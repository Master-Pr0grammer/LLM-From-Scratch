{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run imports\n",
    "import torch, time, Data, random, sys, json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tokenizer import Tokenizer\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "#choose device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Device set to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyper parameters\n",
    "chunk_size = 128\n",
    "embedding_dim = 256 #must be some factor of heads in attention layers\n",
    "\n",
    "# Transforer Hyper parameters\n",
    "num_attention_blocks = 4\n",
    "num_heads = 8\n",
    "dropout_rate = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "train_time = 60 # minutes\n",
    "num_step=15000\n",
    "\n",
    "vocab_file = 'vocab_chars.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "train_file='tbbt_train.txt'\n",
    "test_file='tbbt_test.txt'\n",
    "\n",
    "file = open(vocab_file, 'r')\n",
    "vocab = json.loads(file.read())\n",
    "file.close()\n",
    "\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "data = Data.Data(train_file, test_file, tokenizer, chunk_size, sample_data=True)\n",
    "\n",
    "#sample data\n",
    "x, y = data.get_random_train_sample(num_samples = 1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(f'X: {x.shape}, Y: {y.shape}')\n",
    "print('X: (batch size, chunk size), Y: (batch size, chunk size, num char)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "\n",
    "#load saved checkpoint\n",
    "#model = Transformer.create_from_checkpoint('Transformer.pth.tar').to(device)\n",
    "\n",
    "# Create new model from hyper parameters\n",
    "model = Transformer(\n",
    "    len(tokenizer.vocab_list), \n",
    "    embedding_dim, chunk_size, \n",
    "    num_heads, \n",
    "    num_attention_blocks, \n",
    "    learning_rate, \n",
    "    dropout_rate\n",
    ").to(device)\n",
    "\n",
    "print(model.get_parameter_count(), 'Million Parameters')\n",
    "\n",
    "cost_data, cost_it=[],[]\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load saved checkpoint\n",
    "# model:Transformer = Transformer.create_from_checkpoint('Transformer.pth.tar')\n",
    "# cost_data = model.training_state['cost_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model.train()\n",
    "start=time.time()\n",
    "\n",
    "flatten_const=100\n",
    "\n",
    "while(time.time()-start<60*train_time):\n",
    "    step+=1\n",
    "    x_train, y_train= data.get_random_train_sample(batch_size)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, loss = model(x_train, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "    cost_it.append(loss.item())\n",
    "\n",
    "\n",
    "    #flatten graph data\n",
    "    if (step + 1) % flatten_const == 0:\n",
    "        cost_data.append(torch.tensor(cost_it).mean().item())\n",
    "        cost_it=[]\n",
    "\n",
    "    #print update\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step [{step+1}], Loss: {cost_data[-1]:.4f}\")\n",
    "\n",
    "    if (step + 1) % 5000 == 0:\n",
    "        model.save_checkpoint('Checkpoint.pth.tar', training_state={'cost_data':cost_data})\n",
    "        print('Saved Checkpoint')\n",
    "\n",
    "print(step)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test set\n",
    "model.eval()\n",
    "test_loss_list=[]\n",
    "test_batch_size=64\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range((len(data.test_text)-len(tokenizer.vocab_list)-1)//test_batch_size):\n",
    "        x, y = data.get_test_sample(i*test_batch_size, test_batch_size)\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        outputs, loss = model(x, y)\n",
    "        test_loss_list.append(loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f'Step [{i}/{(len(data.test_text)-len(tokenizer.vocab_list))//test_batch_size}, Loss: {loss.item()}]')\n",
    "val_loss=torch.tensor(test_loss_list).mean().item()\n",
    "print(f'Validation Loss: {val_loss}, Test Time: {time.time() - start}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data\n",
    "plt.plot(torch.arange(0, len(cost_data))*flatten_const, cost_data)\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Average Loss vs Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Inference\n",
    "\n",
    "def sample_with_temp(probs, temperature=1.0, top_k=None):\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = probs ** (1.0 / temperature)\n",
    "    probs = probs / probs.sum()  # Re-normalize\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Get top-k probabilities and their indices\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        top_probs = top_probs / top_probs.sum()\n",
    "        sampled_index = torch.multinomial(top_probs, num_samples=1)\n",
    "        return top_indices[sampled_index].item()\n",
    "    else:\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "# Begin generation\n",
    "model.eval()\n",
    "tokens = data._encoded_train_data[-data.ctx_size:]\n",
    "text = data.train_text[0:data.ctx_size]\n",
    "print('\"', end='', sep='')\n",
    "\n",
    "total_log_prob = 0.0\n",
    "n_tokens = 0\n",
    "\n",
    "# Sampling config\n",
    "TEMPERATURE = 0.8\n",
    "TOP_K = 40\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # Generate input\n",
    "        x = torch.tensor(tokens[-data.ctx_size:]).to(device)\n",
    "        x = x.reshape((1, data.ctx_size))\n",
    "\n",
    "        # Model forward\n",
    "        output = model(x)  # shape: (1, T, vocab_size)\n",
    "        output = output.view((chunk_size, len(tokenizer.vocab_list)))  # Flattening, okay here\n",
    "        logits = output[-1]  # Last token's prediction\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample a token\n",
    "        #sampled_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        sampled_token = sample_with_temp(probs, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "        tokens.append(sampled_token)\n",
    "\n",
    "        # Track log probability\n",
    "        log_prob = torch.log(probs[sampled_token])\n",
    "        total_log_prob += log_prob.item()\n",
    "        n_tokens += 1\n",
    "\n",
    "        # Decode and print\n",
    "        char = tokenizer.decode([sampled_token])[0]\n",
    "        text += char\n",
    "        print(char, end='', sep='')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print('\"\\n', sep='')\n",
    "\n",
    "# Compute and print perplexity\n",
    "import math\n",
    "perplexity = math.exp(-total_log_prob / n_tokens)\n",
    "print(f\"[Perplexity of generated text: {perplexity:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert old model types to new models...\n",
    "\n",
    "# # Save model with hyper parameters for new class definition:\n",
    "# checkpoint = {\n",
    "#     'model_state': model.state_dict(),\n",
    "#     'optimizer_state': model.optimizer.state_dict(),\n",
    "#     'hyperparams': {\n",
    "#         'vocab_size': len(data.tokenizer.vocab_list),\n",
    "#         'ctx_window_length': chunk_size,\n",
    "#         'embedding_dim': embedding_dim,\n",
    "#         'num_attention_blocks': num_attention_blocks,\n",
    "#         'num_attention_heads': num_heads,\n",
    "#         'learning_rate': learning_rate,\n",
    "#         'dropout_rate': dropout_rate\n",
    "#     },\n",
    "#     'training_state':{\n",
    "#         'loss_data':cost_data,\n",
    "#         'val_loss':1.1509664058685303,\n",
    "#         'train_step':78700\n",
    "#     }\n",
    "# }\n",
    "        \n",
    "# torch.save(checkpoint, 'Transformer.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
