{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "#run imports\n",
    "import torch, time, Data, random, sys, json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tokenizer import Tokenizer\n",
    "torch.random.manual_seed(1)\n",
    "\n",
    "#choose device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Device set to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyper parameters\n",
    "chunk_size = 128\n",
    "embedding_dim = 256 #must be some factor of heads in attention layers\n",
    "\n",
    "# Transforer Hyper parameters\n",
    "num_attention_blocks = 4\n",
    "num_heads = 8\n",
    "dropout_rate = 0.0\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "train_time = 60 # minutes\n",
    "num_step=15000\n",
    "\n",
    "vocab_file = 'vocab_chars.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character list:\n",
      "\t['\\n', ' ', '!', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "tensor([[ 0, 25, 33, 44, 37, 47, 43, 32, 33, 27,  0, 47, 31, 33, 42, 33, 23,  1,\n",
      "          1, 29,  1, 31, 43, 46, 46, 37, 32, 43, 46,  1, 29, 48,  1, 29,  1, 47,\n",
      "         44, 33, 46, 41,  1, 30, 29, 42, 39, 11,  0,  0, 47, 36, 33, 40, 32, 43,\n",
      "         42, 23,  1,  1, 47, 43,  1, 37, 34,  1, 29,  1, 44, 36, 43, 48, 43, 42,\n",
      "          1, 37, 47,  1, 32, 37, 46, 33, 31, 48, 33, 32,  1, 48, 36, 46, 43, 49,\n",
      "         35, 36,  1, 29,  1, 44, 40, 29, 42, 33,  1, 51, 37, 48, 36,  1, 48, 51,\n",
      "         43,  1, 47, 40, 37, 48, 47,  1, 37, 42,  1, 37, 48,  1, 29, 42, 32,  1,\n",
      "         33, 37]])\n",
      "tensor([[25, 33, 44, 37, 47, 43, 32, 33, 27,  0, 47, 31, 33, 42, 33, 23,  1,  1,\n",
      "         29,  1, 31, 43, 46, 46, 37, 32, 43, 46,  1, 29, 48,  1, 29,  1, 47, 44,\n",
      "         33, 46, 41,  1, 30, 29, 42, 39, 11,  0,  0, 47, 36, 33, 40, 32, 43, 42,\n",
      "         23,  1,  1, 47, 43,  1, 37, 34,  1, 29,  1, 44, 36, 43, 48, 43, 42,  1,\n",
      "         37, 47,  1, 32, 37, 46, 33, 31, 48, 33, 32,  1, 48, 36, 46, 43, 49, 35,\n",
      "         36,  1, 29,  1, 44, 40, 29, 42, 33,  1, 51, 37, 48, 36,  1, 48, 51, 43,\n",
      "          1, 47, 40, 37, 48, 47,  1, 37, 42,  1, 37, 48,  1, 29, 42, 32,  1, 33,\n",
      "         37, 48]])\n",
      "X: torch.Size([1, 128]), Y: torch.Size([1, 128])\n",
      "X: (batch size, chunk size), Y: (batch size, chunk size, num char)\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "train_file='tbbt_train.txt'\n",
    "test_file='tbbt_test.txt'\n",
    "\n",
    "file = open(vocab_file, 'r')\n",
    "vocab = json.loads(file.read())\n",
    "file.close()\n",
    "\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "data = Data.Data(train_file, test_file, tokenizer, chunk_size, sample_data=True)\n",
    "\n",
    "#sample data\n",
    "x, y = data.get_train_sample(index=0, num_samples = 1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(f'X: {x.shape}, Y: {y.shape}')\n",
    "print('X: (batch size, chunk size), Y: (batch size, chunk size, num char)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transformer network implementation\n",
    "class self_attention_head(nn.Module):\n",
    "    def __init__(self, head_size=16):\n",
    "        super().__init__()\n",
    "        B, T, C = batch_size, chunk_size, embedding_dim\n",
    "        #B = the number of samples in each batch (Batch Size: chunk_size)\n",
    "        #T = the number of items in the time dependant series, (number of characters: num_char)\n",
    "        #C = the number of channels in each time stamp (embedding dimension of time stamp: embedding_dim)\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        #key and query layers\n",
    "        self.key = nn.Linear(C, head_size, bias = False)\n",
    "        self.query = nn.Linear(C, head_size, bias = False)\n",
    "        self.value = nn.Linear(C, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #X = (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        #Get Keys and Qeuries\n",
    "        k = self.key(x) #(B, T, head Size)\n",
    "        q = self.query(x) #(B, T, head Size)\n",
    "\n",
    "        #get averaged weight matrix\n",
    "        #weights = q @ k.transpose(-2, -1) * C ** -0.5 # (B,T,head_size) @ (B,head_size,T) = (B,T,T)\n",
    "        scale = self.head_size ** -0.5\n",
    "        weights = q @ k.transpose(-2, -1) * scale\n",
    "        weights = weights.masked_fill(self.tril == 0, float('-inf')) # optional, when eneabled, it prevents past nodes from accessing future nodes, EX w,o,r,d char w wont have info about o\n",
    "        weights = nn.functional.softmax(weights, dim= -1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        #get output\n",
    "        v = self.value(x)\n",
    "        out = weights @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class multi_head_attention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        if num_heads * head_size != embedding_dim:\n",
    "            raise ValueError(\"num_heads * head_size must equal embedding_dim\")\n",
    "\n",
    "        self.heads = nn.ModuleList([self_attention_head(head_size=head_size) for i in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class Attention_Block(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = embedding_dim // num_heads\n",
    "        if self.head_size * num_heads != embedding_dim:\n",
    "            raise ValueError(\"embedding dimenstion needs to be divisible by number of heads\")\n",
    "        self.attention = multi_head_attention(num_heads, head_size=self.head_size)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        #feed forward section\n",
    "        self.ffw = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm1(x))\n",
    "        x = x + self.ffw(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "#want an output of B, embedding dim\n",
    "B, T, C = batch_size, chunk_size, embedding_dim\n",
    "x= torch.randn(B,T,C).to(device)\n",
    "attention_head=Attention_Block(embedding_dim, 4).to(device)\n",
    "out = attention_head.forward(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.217463 Million Parameters\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network model\n",
    "class Brain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Brain, self).__init__()\n",
    "\n",
    "        self.char_emb = nn.Embedding(len(tokenizer.vocab_list), embedding_dim)\n",
    "        self.pos_emb = nn.Embedding(chunk_size, embedding_dim)\n",
    "\n",
    "        #attention layers\n",
    "        self.attention_blocks = nn.Sequential(*[Attention_Block(embedding_dim, num_heads) for i in range(num_attention_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        #Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, len(tokenizer.vocab_list))\n",
    "\n",
    "        #better weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.optimizer= torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, targets= None):\n",
    "        #run through embedding layer\n",
    "        B, T = x.size()\n",
    "        C = len(tokenizer.vocab_list)\n",
    "        \n",
    "        char_emb = self.char_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(chunk_size, device=device))\n",
    "        x = char_emb + pos_emb\n",
    "\n",
    "        x = self.attention_blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        #x = x.view((-1, chunk_size * embedding_dim))\n",
    "\n",
    "        #run through reshaping layer and softmax\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        if targets is None:\n",
    "            #logits = self.softmax(output)\n",
    "            return logits\n",
    "        else:\n",
    "            #print(logits.size())\n",
    "            logits_flat  = logits.view(B * T, C)     # (64*32, 55)\n",
    "            targets_flat = targets.view(B * T)       # (64*32)\n",
    "            #print(logits_flat.size(),targets_flat.size())\n",
    "            loss = self.loss_func(logits_flat, targets_flat)\n",
    "            \n",
    "            # logits = logits.view(batch_size, -1)\n",
    "            # print(logits.size(), targets.size())\n",
    "            # loss = self.loss_func(logits, targets)\n",
    "            # print(loss.item())\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -chunk_size:]\n",
    "            # get the predictions\n",
    "            logits = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            log_probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(log_probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Create the model\n",
    "model = Brain()\n",
    "model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1E6, 'Million Parameters')\n",
    "\n",
    "cost_data, cost_it=[],[]\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved checkpoint\n",
    "# checkpoint_state = torch.load('Checkpoint.pth.tar')\n",
    "# model.load_state_dict(checkpoint_state['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint_state['optimizer'])\n",
    "# cost_data = checkpoint_state['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [50500], Loss: 1.2077\n",
      "Step [50600], Loss: 1.2518\n",
      "Step [50700], Loss: 1.2435\n",
      "Step [50800], Loss: 1.2429\n",
      "Step [50900], Loss: 1.2015\n",
      "Step [51000], Loss: 1.2204\n",
      "Step [51100], Loss: 1.2046\n",
      "Step [51200], Loss: 1.1916\n",
      "Step [51300], Loss: 1.2099\n",
      "Step [51400], Loss: 1.1915\n",
      "Step [51500], Loss: 1.2054\n",
      "Step [51600], Loss: 1.1991\n",
      "Step [51700], Loss: 1.2163\n",
      "Step [51800], Loss: 1.2673\n",
      "Step [51900], Loss: 1.2380\n",
      "Step [52000], Loss: 1.1961\n",
      "Step [52100], Loss: 1.2013\n",
      "Step [52200], Loss: 1.1958\n",
      "Step [52300], Loss: 1.1956\n",
      "Step [52400], Loss: 1.2010\n",
      "Step [52500], Loss: 1.1950\n",
      "Step [52600], Loss: 1.2187\n",
      "Step [52700], Loss: 1.1995\n",
      "Step [52800], Loss: 1.2076\n",
      "Step [52900], Loss: 1.2178\n",
      "Step [53000], Loss: 1.1901\n",
      "Step [53100], Loss: 1.2010\n",
      "Step [53200], Loss: 1.2322\n",
      "Step [53300], Loss: 1.1608\n",
      "Step [53400], Loss: 1.1824\n",
      "Step [53500], Loss: 1.2027\n",
      "Step [53600], Loss: 1.1849\n",
      "Step [53700], Loss: 1.2220\n",
      "Step [53800], Loss: 1.2084\n",
      "Step [53900], Loss: 1.2060\n",
      "Step [54000], Loss: 1.1800\n",
      "Step [54100], Loss: 1.2098\n",
      "Step [54200], Loss: 1.1794\n",
      "Step [54300], Loss: 1.1799\n",
      "Step [54400], Loss: 1.1582\n",
      "Step [54500], Loss: 1.2166\n",
      "Step [54600], Loss: 1.1869\n",
      "Step [54700], Loss: 1.2265\n",
      "Step [54800], Loss: 1.1913\n",
      "Step [54900], Loss: 1.1608\n",
      "Step [55000], Loss: 1.2038\n",
      "Saved Checkpoint\n",
      "Step [55100], Loss: 1.1631\n",
      "Step [55200], Loss: 1.1944\n",
      "Step [55300], Loss: 1.2132\n",
      "Step [55400], Loss: 1.2173\n",
      "Step [55500], Loss: 1.2141\n",
      "Step [55600], Loss: 1.2282\n",
      "Step [55700], Loss: 1.1795\n",
      "Step [55800], Loss: 1.2096\n",
      "Step [55900], Loss: 1.2721\n",
      "Step [56000], Loss: 1.2057\n",
      "Step [56100], Loss: 1.1895\n",
      "Step [56200], Loss: 1.1627\n",
      "Step [56300], Loss: 1.2306\n",
      "Step [56400], Loss: 1.1660\n",
      "Step [56500], Loss: 1.2051\n",
      "Step [56600], Loss: 1.1843\n",
      "Step [56700], Loss: 1.2330\n",
      "Step [56800], Loss: 1.2462\n",
      "Step [56900], Loss: 1.2062\n",
      "Step [57000], Loss: 1.1957\n",
      "Step [57100], Loss: 1.1821\n",
      "Step [57200], Loss: 1.1993\n",
      "Step [57300], Loss: 1.2336\n",
      "Step [57400], Loss: 1.2451\n",
      "Step [57500], Loss: 1.1965\n",
      "Step [57600], Loss: 1.1796\n",
      "Step [57700], Loss: 1.1849\n",
      "Step [57800], Loss: 1.2108\n",
      "Step [57900], Loss: 1.2326\n",
      "Step [58000], Loss: 1.2156\n",
      "Step [58100], Loss: 1.2023\n",
      "Step [58200], Loss: 1.2222\n",
      "Step [58300], Loss: 1.2129\n",
      "Step [58400], Loss: 1.1807\n",
      "Step [58500], Loss: 1.1706\n",
      "Step [58600], Loss: 1.1547\n",
      "Step [58700], Loss: 1.2011\n",
      "Step [58800], Loss: 1.1568\n",
      "Step [58900], Loss: 1.2250\n",
      "Step [59000], Loss: 1.1599\n",
      "Step [59100], Loss: 1.2140\n",
      "Step [59200], Loss: 1.2654\n",
      "Step [59300], Loss: 1.1817\n",
      "Step [59400], Loss: 1.2042\n",
      "Step [59500], Loss: 1.1980\n",
      "Step [59600], Loss: 1.1843\n",
      "Step [59700], Loss: 1.1723\n",
      "Step [59800], Loss: 1.2042\n",
      "Step [59900], Loss: 1.2304\n",
      "Step [60000], Loss: 1.1939\n",
      "Saved Checkpoint\n",
      "Step [60100], Loss: 1.2059\n",
      "Step [60200], Loss: 1.1747\n",
      "Step [60300], Loss: 1.2184\n",
      "Step [60400], Loss: 1.2090\n",
      "Step [60500], Loss: 1.1961\n",
      "Step [60600], Loss: 1.2135\n",
      "Step [60700], Loss: 1.1911\n",
      "Step [60800], Loss: 1.2190\n",
      "Step [60900], Loss: 1.1566\n",
      "Step [61000], Loss: 1.2269\n",
      "Step [61100], Loss: 1.1571\n",
      "Step [61200], Loss: 1.2500\n",
      "Step [61300], Loss: 1.1961\n",
      "Step [61400], Loss: 1.1870\n",
      "Step [61500], Loss: 1.2012\n",
      "Step [61600], Loss: 1.1759\n",
      "Step [61700], Loss: 1.1420\n",
      "Step [61800], Loss: 1.2008\n",
      "Step [61900], Loss: 1.1936\n",
      "Step [62000], Loss: 1.2043\n",
      "Step [62100], Loss: 1.1920\n",
      "Step [62200], Loss: 1.2135\n",
      "Step [62300], Loss: 1.2020\n",
      "Step [62400], Loss: 1.1775\n",
      "Step [62500], Loss: 1.1637\n",
      "Step [62600], Loss: 1.2446\n",
      "Step [62700], Loss: 1.1451\n",
      "Step [62800], Loss: 1.1797\n",
      "Step [62900], Loss: 1.1975\n",
      "Step [63000], Loss: 1.1707\n",
      "Step [63100], Loss: 1.1683\n",
      "Step [63200], Loss: 1.1975\n",
      "Step [63300], Loss: 1.1380\n",
      "Step [63400], Loss: 1.1880\n",
      "Step [63500], Loss: 1.1626\n",
      "Step [63600], Loss: 1.2334\n",
      "Step [63700], Loss: 1.1852\n",
      "Step [63800], Loss: 1.1494\n",
      "Step [63900], Loss: 1.1711\n",
      "Step [64000], Loss: 1.1670\n",
      "Step [64100], Loss: 1.1698\n",
      "Step [64200], Loss: 1.1716\n",
      "Step [64300], Loss: 1.1816\n",
      "Step [64400], Loss: 1.1646\n",
      "Step [64500], Loss: 1.1982\n",
      "Step [64600], Loss: 1.2078\n",
      "Step [64700], Loss: 1.1860\n",
      "Step [64800], Loss: 1.1400\n",
      "Step [64900], Loss: 1.1583\n",
      "Step [65000], Loss: 1.1613\n",
      "Saved Checkpoint\n",
      "Step [65100], Loss: 1.1475\n",
      "Step [65200], Loss: 1.1790\n",
      "Step [65300], Loss: 1.1840\n",
      "Step [65400], Loss: 1.1946\n",
      "Step [65500], Loss: 1.1548\n",
      "Step [65600], Loss: 1.2034\n",
      "Step [65700], Loss: 1.1732\n",
      "Step [65800], Loss: 1.1798\n",
      "Step [65900], Loss: 1.1746\n",
      "Step [66000], Loss: 1.1428\n",
      "Step [66100], Loss: 1.1772\n",
      "Step [66200], Loss: 1.1693\n",
      "Step [66300], Loss: 1.1918\n",
      "Step [66400], Loss: 1.2094\n",
      "Step [66500], Loss: 1.1927\n",
      "Step [66600], Loss: 1.1979\n",
      "Step [66700], Loss: 1.2180\n",
      "Step [66800], Loss: 1.1304\n",
      "Step [66900], Loss: 1.1984\n",
      "Step [67000], Loss: 1.1869\n",
      "Step [67100], Loss: 1.1931\n",
      "Step [67200], Loss: 1.1880\n",
      "Step [67300], Loss: 1.2136\n",
      "Step [67400], Loss: 1.1534\n",
      "Step [67500], Loss: 1.1864\n",
      "Step [67600], Loss: 1.1921\n",
      "Step [67700], Loss: 1.2067\n",
      "Step [67800], Loss: 1.1753\n",
      "Step [67900], Loss: 1.1912\n",
      "Step [68000], Loss: 1.1436\n",
      "Step [68100], Loss: 1.1882\n",
      "Step [68200], Loss: 1.1547\n",
      "Step [68300], Loss: 1.1722\n",
      "Step [68400], Loss: 1.1361\n",
      "Step [68500], Loss: 1.1661\n",
      "Step [68600], Loss: 1.1570\n",
      "Step [68700], Loss: 1.2065\n",
      "Step [68800], Loss: 1.2054\n",
      "Step [68900], Loss: 1.1736\n",
      "Step [69000], Loss: 1.2229\n",
      "Step [69100], Loss: 1.1942\n",
      "Step [69200], Loss: 1.1867\n",
      "Step [69300], Loss: 1.1777\n",
      "Step [69400], Loss: 1.1659\n",
      "Step [69500], Loss: 1.1874\n",
      "Step [69600], Loss: 1.1829\n",
      "Step [69700], Loss: 1.1318\n",
      "Step [69800], Loss: 1.1414\n",
      "Step [69900], Loss: 1.1891\n",
      "Step [70000], Loss: 1.2003\n",
      "Saved Checkpoint\n",
      "Step [70100], Loss: 1.1704\n",
      "Step [70200], Loss: 1.2004\n",
      "Step [70300], Loss: 1.1737\n",
      "Step [70400], Loss: 1.2124\n",
      "Step [70500], Loss: 1.1960\n",
      "Step [70600], Loss: 1.1831\n",
      "Step [70700], Loss: 1.1924\n",
      "Step [70800], Loss: 1.1514\n",
      "Step [70900], Loss: 1.1539\n",
      "Step [71000], Loss: 1.1999\n",
      "Step [71100], Loss: 1.1456\n",
      "Step [71200], Loss: 1.2167\n",
      "Step [71300], Loss: 1.1549\n",
      "Step [71400], Loss: 1.2152\n",
      "Step [71500], Loss: 1.1723\n",
      "Step [71600], Loss: 1.1994\n",
      "Step [71700], Loss: 1.1687\n",
      "Step [71800], Loss: 1.1676\n",
      "Step [71900], Loss: 1.2080\n",
      "Step [72000], Loss: 1.1549\n",
      "Step [72100], Loss: 1.1685\n",
      "Step [72200], Loss: 1.1602\n",
      "Step [72300], Loss: 1.1673\n",
      "Step [72400], Loss: 1.1655\n",
      "Step [72500], Loss: 1.1665\n",
      "Step [72600], Loss: 1.1463\n",
      "Step [72700], Loss: 1.1680\n",
      "Step [72800], Loss: 1.1614\n",
      "Step [72900], Loss: 1.1862\n",
      "Step [73000], Loss: 1.1471\n",
      "Step [73100], Loss: 1.1964\n",
      "Step [73200], Loss: 1.1309\n",
      "Step [73300], Loss: 1.2118\n",
      "Step [73400], Loss: 1.1550\n",
      "Step [73500], Loss: 1.1596\n",
      "Step [73600], Loss: 1.1967\n",
      "Step [73700], Loss: 1.1848\n",
      "Step [73800], Loss: 1.1428\n",
      "Step [73900], Loss: 1.1791\n",
      "Step [74000], Loss: 1.1600\n",
      "Step [74100], Loss: 1.1841\n",
      "Step [74200], Loss: 1.1556\n",
      "Step [74300], Loss: 1.2220\n",
      "Step [74400], Loss: 1.1467\n",
      "Step [74500], Loss: 1.2092\n",
      "Step [74600], Loss: 1.1902\n",
      "Step [74700], Loss: 1.1699\n",
      "Step [74800], Loss: 1.1667\n",
      "Step [74900], Loss: 1.2302\n",
      "Step [75000], Loss: 1.1781\n",
      "Saved Checkpoint\n",
      "Step [75100], Loss: 1.1820\n",
      "Step [75200], Loss: 1.1532\n",
      "Step [75300], Loss: 1.1641\n",
      "Step [75400], Loss: 1.1544\n",
      "Step [75500], Loss: 1.1913\n",
      "Step [75600], Loss: 1.1552\n",
      "Step [75700], Loss: 1.1634\n",
      "Step [75800], Loss: 1.1646\n",
      "Step [75900], Loss: 1.1584\n",
      "Step [76000], Loss: 1.1520\n",
      "Step [76100], Loss: 1.1614\n",
      "Step [76200], Loss: 1.1436\n",
      "Step [76300], Loss: 1.1285\n",
      "Step [76400], Loss: 1.1698\n",
      "Step [76500], Loss: 1.1902\n",
      "Step [76600], Loss: 1.1789\n",
      "Step [76700], Loss: 1.1414\n",
      "Step [76800], Loss: 1.2016\n",
      "Step [76900], Loss: 1.1659\n",
      "Step [77000], Loss: 1.1493\n",
      "Step [77100], Loss: 1.1642\n",
      "Step [77200], Loss: 1.1567\n",
      "Step [77300], Loss: 1.1725\n",
      "Step [77400], Loss: 1.1684\n",
      "Step [77500], Loss: 1.1344\n",
      "Step [77600], Loss: 1.1396\n",
      "Step [77700], Loss: 1.1523\n",
      "Step [77800], Loss: 1.1293\n",
      "Step [77900], Loss: 1.1599\n",
      "Step [78000], Loss: 1.1576\n",
      "Step [78100], Loss: 1.1476\n",
      "Step [78200], Loss: 1.1637\n",
      "Step [78300], Loss: 1.1307\n",
      "Step [78400], Loss: 1.1675\n",
      "Step [78500], Loss: 1.1354\n",
      "Step [78600], Loss: 1.1724\n",
      "Step [78700], Loss: 1.1890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[32m     19\u001b[39m model.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m model.optimizer.step()\n\u001b[32m     23\u001b[39m cost_it.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/LLM-From-Scratch/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/LLM-From-Scratch/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/LLM-From-Scratch/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "model.train()\n",
    "start=time.time()\n",
    "\n",
    "flatten_const=100\n",
    "\n",
    "while(time.time()-start<60*train_time):\n",
    "    step+=1\n",
    "    #for step in range(num_step):\n",
    "    index = random.randint(0, len(data.train_text) - data.ctx_size - 1 - batch_size)\n",
    "    x_train, y_train= data.get_train_sample(index, batch_size)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs, loss = model(x_train, y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "\n",
    "    cost_it.append(loss.item())\n",
    "\n",
    "\n",
    "    #flatten graph data\n",
    "    if (step + 1) % flatten_const == 0:\n",
    "        cost_data.append(torch.tensor(cost_it).mean().item())\n",
    "        cost_it=[]\n",
    "\n",
    "    #print update\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step [{step+1}], Loss: {cost_data[-1]:.4f}\")\n",
    "\n",
    "    if (step + 1) % 5000 == 0:\n",
    "        state = {'state_dict':model.state_dict(), 'optimizer':model.optimizer.state_dict(), 'loss':cost_data}\n",
    "        torch.save(state, 'Checkpoint.pth.tar')\n",
    "        print('Saved Checkpoint')\n",
    "\n",
    "print(step)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/362, Loss: 1.1128332614898682]\n",
      "Step [1/362, Loss: 1.1247279644012451]\n",
      "Step [2/362, Loss: 1.110111951828003]\n",
      "Step [3/362, Loss: 1.0498850345611572]\n",
      "Step [4/362, Loss: 1.1062884330749512]\n",
      "Step [5/362, Loss: 1.128397822380066]\n",
      "Step [6/362, Loss: 1.185092806816101]\n",
      "Step [7/362, Loss: 1.093511700630188]\n",
      "Step [8/362, Loss: 1.1483087539672852]\n",
      "Step [9/362, Loss: 1.3549158573150635]\n",
      "Step [10/362, Loss: 1.1504334211349487]\n",
      "Step [11/362, Loss: 1.1783664226531982]\n",
      "Step [12/362, Loss: 1.143185019493103]\n",
      "Step [13/362, Loss: 1.216731309890747]\n",
      "Step [14/362, Loss: 1.2527354955673218]\n",
      "Step [15/362, Loss: 1.3713186979293823]\n",
      "Step [16/362, Loss: 1.1141513586044312]\n",
      "Step [17/362, Loss: 1.169584035873413]\n",
      "Step [18/362, Loss: 1.3963414430618286]\n",
      "Step [19/362, Loss: 1.2054692506790161]\n",
      "Step [20/362, Loss: 1.145362138748169]\n",
      "Step [21/362, Loss: 1.1534744501113892]\n",
      "Step [22/362, Loss: 1.142699122428894]\n",
      "Step [23/362, Loss: 1.2082562446594238]\n",
      "Step [24/362, Loss: 1.2678192853927612]\n",
      "Step [25/362, Loss: 1.440969705581665]\n",
      "Step [26/362, Loss: 1.4243987798690796]\n",
      "Step [27/362, Loss: 1.1119093894958496]\n",
      "Step [28/362, Loss: 1.141353964805603]\n",
      "Step [29/362, Loss: 1.056920051574707]\n",
      "Step [30/362, Loss: 1.0720077753067017]\n",
      "Step [31/362, Loss: 1.1619899272918701]\n",
      "Step [32/362, Loss: 1.151360273361206]\n",
      "Step [33/362, Loss: 1.1164753437042236]\n",
      "Step [34/362, Loss: 1.294948697090149]\n",
      "Step [35/362, Loss: 1.2231730222702026]\n",
      "Step [36/362, Loss: 1.1658880710601807]\n",
      "Step [37/362, Loss: 1.2150988578796387]\n",
      "Step [38/362, Loss: 1.0502879619598389]\n",
      "Step [39/362, Loss: 1.217892050743103]\n",
      "Step [40/362, Loss: 1.1400139331817627]\n",
      "Step [41/362, Loss: 1.255936861038208]\n",
      "Step [42/362, Loss: 1.1200575828552246]\n",
      "Step [43/362, Loss: 1.2671458721160889]\n",
      "Step [44/362, Loss: 1.1917632818222046]\n",
      "Step [45/362, Loss: 1.0825053453445435]\n",
      "Step [46/362, Loss: 1.1435651779174805]\n",
      "Step [47/362, Loss: 1.2389153242111206]\n",
      "Step [48/362, Loss: 1.144683837890625]\n",
      "Step [49/362, Loss: 1.1950640678405762]\n",
      "Step [50/362, Loss: 1.2216144800186157]\n",
      "Step [51/362, Loss: 1.130131721496582]\n",
      "Step [52/362, Loss: 1.1809552907943726]\n",
      "Step [53/362, Loss: 1.1151777505874634]\n",
      "Step [54/362, Loss: 1.3520605564117432]\n",
      "Step [55/362, Loss: 1.2057946920394897]\n",
      "Step [56/362, Loss: 1.1874206066131592]\n",
      "Step [57/362, Loss: 1.242996335029602]\n",
      "Step [58/362, Loss: 1.1541781425476074]\n",
      "Step [59/362, Loss: 1.086629867553711]\n",
      "Step [60/362, Loss: 1.2591705322265625]\n",
      "Step [61/362, Loss: 1.3896397352218628]\n",
      "Step [62/362, Loss: 1.0853102207183838]\n",
      "Step [63/362, Loss: 1.2584738731384277]\n",
      "Step [64/362, Loss: 1.257466197013855]\n",
      "Step [65/362, Loss: 1.173215627670288]\n",
      "Step [66/362, Loss: 1.1494206190109253]\n",
      "Step [67/362, Loss: 1.1924012899398804]\n",
      "Step [68/362, Loss: 1.3786122798919678]\n",
      "Step [69/362, Loss: 1.1699870824813843]\n",
      "Step [70/362, Loss: 1.207566261291504]\n",
      "Step [71/362, Loss: 1.1504884958267212]\n",
      "Step [72/362, Loss: 1.1135954856872559]\n",
      "Step [73/362, Loss: 1.0481871366500854]\n",
      "Step [74/362, Loss: 1.1345051527023315]\n",
      "Step [75/362, Loss: 1.0542452335357666]\n",
      "Step [76/362, Loss: 1.0288920402526855]\n",
      "Step [77/362, Loss: 1.0142085552215576]\n",
      "Step [78/362, Loss: 1.0968819856643677]\n",
      "Step [79/362, Loss: 1.1055575609207153]\n",
      "Step [80/362, Loss: 1.0904960632324219]\n",
      "Step [81/362, Loss: 1.2518490552902222]\n",
      "Step [82/362, Loss: 1.1797220706939697]\n",
      "Step [83/362, Loss: 1.108948826789856]\n",
      "Step [84/362, Loss: 1.1355222463607788]\n",
      "Step [85/362, Loss: 1.242121696472168]\n",
      "Step [86/362, Loss: 1.1695698499679565]\n",
      "Step [87/362, Loss: 1.1787614822387695]\n",
      "Step [88/362, Loss: 1.1757272481918335]\n",
      "Step [89/362, Loss: 1.0459163188934326]\n",
      "Step [90/362, Loss: 1.1461104154586792]\n",
      "Step [91/362, Loss: 1.110250473022461]\n",
      "Step [92/362, Loss: 1.1097217798233032]\n",
      "Step [93/362, Loss: 1.1524244546890259]\n",
      "Step [94/362, Loss: 1.167688012123108]\n",
      "Step [95/362, Loss: 1.2831292152404785]\n",
      "Step [96/362, Loss: 1.2893211841583252]\n",
      "Step [97/362, Loss: 1.3217653036117554]\n",
      "Step [98/362, Loss: 1.239118218421936]\n",
      "Step [99/362, Loss: 1.1292006969451904]\n",
      "Step [100/362, Loss: 1.0160435438156128]\n",
      "Step [101/362, Loss: 1.216752052307129]\n",
      "Step [102/362, Loss: 1.113062858581543]\n",
      "Step [103/362, Loss: 1.0618014335632324]\n",
      "Step [104/362, Loss: 1.0847936868667603]\n",
      "Step [105/362, Loss: 1.1153159141540527]\n",
      "Step [106/362, Loss: 1.1910752058029175]\n",
      "Step [107/362, Loss: 1.2326593399047852]\n",
      "Step [108/362, Loss: 1.1865448951721191]\n",
      "Step [109/362, Loss: 1.1638060808181763]\n",
      "Step [110/362, Loss: 1.1239558458328247]\n",
      "Step [111/362, Loss: 1.1201056241989136]\n",
      "Step [112/362, Loss: 1.0652246475219727]\n",
      "Step [113/362, Loss: 1.1191442012786865]\n",
      "Step [114/362, Loss: 1.1701663732528687]\n",
      "Step [115/362, Loss: 1.1452263593673706]\n",
      "Step [116/362, Loss: 1.1598211526870728]\n",
      "Step [117/362, Loss: 1.2196612358093262]\n",
      "Step [118/362, Loss: 1.0278409719467163]\n",
      "Step [119/362, Loss: 1.1789076328277588]\n",
      "Step [120/362, Loss: 1.2314653396606445]\n",
      "Step [121/362, Loss: 1.1871153116226196]\n",
      "Step [122/362, Loss: 1.1071832180023193]\n",
      "Step [123/362, Loss: 1.0710914134979248]\n",
      "Step [124/362, Loss: 1.1303960084915161]\n",
      "Step [125/362, Loss: 1.211273193359375]\n",
      "Step [126/362, Loss: 1.2414908409118652]\n",
      "Step [127/362, Loss: 1.1791343688964844]\n",
      "Step [128/362, Loss: 1.0815088748931885]\n",
      "Step [129/362, Loss: 1.0995078086853027]\n",
      "Step [130/362, Loss: 1.0791441202163696]\n",
      "Step [131/362, Loss: 1.1308692693710327]\n",
      "Step [132/362, Loss: 1.187941312789917]\n",
      "Step [133/362, Loss: 1.162551999092102]\n",
      "Step [134/362, Loss: 1.1673578023910522]\n",
      "Step [135/362, Loss: 1.2289607524871826]\n",
      "Step [136/362, Loss: 1.1638702154159546]\n",
      "Step [137/362, Loss: 1.0680314302444458]\n",
      "Step [138/362, Loss: 1.1141548156738281]\n",
      "Step [139/362, Loss: 1.2555372714996338]\n",
      "Step [140/362, Loss: 1.0487116575241089]\n",
      "Step [141/362, Loss: 1.1236804723739624]\n",
      "Step [142/362, Loss: 1.123930811882019]\n",
      "Step [143/362, Loss: 1.1005107164382935]\n",
      "Step [144/362, Loss: 1.0983363389968872]\n",
      "Step [145/362, Loss: 1.048267126083374]\n",
      "Step [146/362, Loss: 1.0961740016937256]\n",
      "Step [147/362, Loss: 0.9763022661209106]\n",
      "Step [148/362, Loss: 1.2210471630096436]\n",
      "Step [149/362, Loss: 1.1324714422225952]\n",
      "Step [150/362, Loss: 1.118177056312561]\n",
      "Step [151/362, Loss: 1.1464757919311523]\n",
      "Step [152/362, Loss: 1.0234529972076416]\n",
      "Step [153/362, Loss: 1.190189003944397]\n",
      "Step [154/362, Loss: 1.0932855606079102]\n",
      "Step [155/362, Loss: 1.0986931324005127]\n",
      "Step [156/362, Loss: 1.161523461341858]\n",
      "Step [157/362, Loss: 1.1459764242172241]\n",
      "Step [158/362, Loss: 1.2741539478302002]\n",
      "Step [159/362, Loss: 1.0958285331726074]\n",
      "Step [160/362, Loss: 1.2747888565063477]\n",
      "Step [161/362, Loss: 1.1784111261367798]\n",
      "Step [162/362, Loss: 1.2274218797683716]\n",
      "Step [163/362, Loss: 1.2290760278701782]\n",
      "Step [164/362, Loss: 1.1255571842193604]\n",
      "Step [165/362, Loss: 1.1294543743133545]\n",
      "Step [166/362, Loss: 1.1970380544662476]\n",
      "Step [167/362, Loss: 1.1846046447753906]\n",
      "Step [168/362, Loss: 1.1651742458343506]\n",
      "Step [169/362, Loss: 1.1801003217697144]\n",
      "Step [170/362, Loss: 1.1502985954284668]\n",
      "Step [171/362, Loss: 1.2314282655715942]\n",
      "Step [172/362, Loss: 1.1751235723495483]\n",
      "Step [173/362, Loss: 1.093654990196228]\n",
      "Step [174/362, Loss: 1.1556529998779297]\n",
      "Step [175/362, Loss: 1.1938183307647705]\n",
      "Step [176/362, Loss: 1.2312273979187012]\n",
      "Step [177/362, Loss: 1.1318583488464355]\n",
      "Step [178/362, Loss: 1.27741277217865]\n",
      "Step [179/362, Loss: 1.1158267259597778]\n",
      "Step [180/362, Loss: 1.2057517766952515]\n",
      "Step [181/362, Loss: 1.0535387992858887]\n",
      "Step [182/362, Loss: 1.1275475025177002]\n",
      "Step [183/362, Loss: 1.1892421245574951]\n",
      "Step [184/362, Loss: 1.181552767753601]\n",
      "Step [185/362, Loss: 1.2354310750961304]\n",
      "Step [186/362, Loss: 1.2048511505126953]\n",
      "Step [187/362, Loss: 1.1632628440856934]\n",
      "Step [188/362, Loss: 1.1341166496276855]\n",
      "Step [189/362, Loss: 1.1994580030441284]\n",
      "Step [190/362, Loss: 1.1938865184783936]\n",
      "Step [191/362, Loss: 1.151486873626709]\n",
      "Step [192/362, Loss: 1.2228080034255981]\n",
      "Step [193/362, Loss: 1.1716219186782837]\n",
      "Step [194/362, Loss: 1.2138597965240479]\n",
      "Step [195/362, Loss: 1.1523432731628418]\n",
      "Step [196/362, Loss: 1.0784943103790283]\n",
      "Step [197/362, Loss: 1.1128685474395752]\n",
      "Step [198/362, Loss: 1.150871753692627]\n",
      "Step [199/362, Loss: 0.999862551689148]\n",
      "Step [200/362, Loss: 1.1835417747497559]\n",
      "Step [201/362, Loss: 1.2223976850509644]\n",
      "Step [202/362, Loss: 1.089669942855835]\n",
      "Step [203/362, Loss: 1.1411629915237427]\n",
      "Step [204/362, Loss: 1.1347723007202148]\n",
      "Step [205/362, Loss: 1.1232600212097168]\n",
      "Step [206/362, Loss: 1.1984378099441528]\n",
      "Step [207/362, Loss: 1.1823734045028687]\n",
      "Step [208/362, Loss: 1.1752574443817139]\n",
      "Step [209/362, Loss: 1.069735050201416]\n",
      "Step [210/362, Loss: 1.1672316789627075]\n",
      "Step [211/362, Loss: 1.0757672786712646]\n",
      "Step [212/362, Loss: 1.2097309827804565]\n",
      "Step [213/362, Loss: 0.925711989402771]\n",
      "Step [214/362, Loss: 1.0650644302368164]\n",
      "Step [215/362, Loss: 1.049996256828308]\n",
      "Step [216/362, Loss: 1.1530076265335083]\n",
      "Step [217/362, Loss: 1.0327340364456177]\n",
      "Step [218/362, Loss: 1.1122462749481201]\n",
      "Step [219/362, Loss: 1.1193454265594482]\n",
      "Step [220/362, Loss: 1.1272218227386475]\n",
      "Step [221/362, Loss: 1.0474605560302734]\n",
      "Step [222/362, Loss: 1.0380909442901611]\n",
      "Step [223/362, Loss: 1.1886470317840576]\n",
      "Step [224/362, Loss: 1.157967448234558]\n",
      "Step [225/362, Loss: 1.2471508979797363]\n",
      "Step [226/362, Loss: 1.2020243406295776]\n",
      "Step [227/362, Loss: 1.2001453638076782]\n",
      "Step [228/362, Loss: 1.213022232055664]\n",
      "Step [229/362, Loss: 1.1379525661468506]\n",
      "Step [230/362, Loss: 1.2256056070327759]\n",
      "Step [231/362, Loss: 1.267069697380066]\n",
      "Step [232/362, Loss: 1.0645498037338257]\n",
      "Step [233/362, Loss: 1.160344123840332]\n",
      "Step [234/362, Loss: 1.3635952472686768]\n",
      "Step [235/362, Loss: 1.0099384784698486]\n",
      "Step [236/362, Loss: 1.11643385887146]\n",
      "Step [237/362, Loss: 1.1740212440490723]\n",
      "Step [238/362, Loss: 1.3088752031326294]\n",
      "Step [239/362, Loss: 1.2601969242095947]\n",
      "Step [240/362, Loss: 1.1440333127975464]\n",
      "Step [241/362, Loss: 1.320273995399475]\n",
      "Step [242/362, Loss: 1.142427682876587]\n",
      "Step [243/362, Loss: 1.2183383703231812]\n",
      "Step [244/362, Loss: 1.2721366882324219]\n",
      "Step [245/362, Loss: 1.1559629440307617]\n",
      "Step [246/362, Loss: 1.1295702457427979]\n",
      "Step [247/362, Loss: 1.0873600244522095]\n",
      "Step [248/362, Loss: 1.1958301067352295]\n",
      "Step [249/362, Loss: 1.1444287300109863]\n",
      "Step [250/362, Loss: 1.0990898609161377]\n",
      "Step [251/362, Loss: 1.1535944938659668]\n",
      "Step [252/362, Loss: 1.1302146911621094]\n",
      "Step [253/362, Loss: 1.127238154411316]\n",
      "Step [254/362, Loss: 1.1220793724060059]\n",
      "Step [255/362, Loss: 1.0758817195892334]\n",
      "Step [256/362, Loss: 1.1887189149856567]\n",
      "Step [257/362, Loss: 1.1465238332748413]\n",
      "Step [258/362, Loss: 1.1485090255737305]\n",
      "Step [259/362, Loss: 1.1403592824935913]\n",
      "Step [260/362, Loss: 1.0727072954177856]\n",
      "Step [261/362, Loss: 1.3310390710830688]\n",
      "Step [262/362, Loss: 1.0963757038116455]\n",
      "Step [263/362, Loss: 1.0818095207214355]\n",
      "Step [264/362, Loss: 1.1759642362594604]\n",
      "Step [265/362, Loss: 1.1510519981384277]\n",
      "Step [266/362, Loss: 1.1123929023742676]\n",
      "Step [267/362, Loss: 1.1383763551712036]\n",
      "Step [268/362, Loss: 1.1496728658676147]\n",
      "Step [269/362, Loss: 1.2428334951400757]\n",
      "Step [270/362, Loss: 1.1446856260299683]\n",
      "Step [271/362, Loss: 1.1074929237365723]\n",
      "Step [272/362, Loss: 1.1383942365646362]\n",
      "Step [273/362, Loss: 1.0944230556488037]\n",
      "Step [274/362, Loss: 1.0498969554901123]\n",
      "Step [275/362, Loss: 1.1616629362106323]\n",
      "Step [276/362, Loss: 1.0832058191299438]\n",
      "Step [277/362, Loss: 1.0016056299209595]\n",
      "Step [278/362, Loss: 1.1770809888839722]\n",
      "Step [279/362, Loss: 1.2553044557571411]\n",
      "Step [280/362, Loss: 1.071635365486145]\n",
      "Step [281/362, Loss: 1.1983979940414429]\n",
      "Step [282/362, Loss: 1.036885142326355]\n",
      "Step [283/362, Loss: 1.0599703788757324]\n",
      "Step [284/362, Loss: 1.1375796794891357]\n",
      "Step [285/362, Loss: 1.0924943685531616]\n",
      "Step [286/362, Loss: 1.0154733657836914]\n",
      "Step [287/362, Loss: 1.1544080972671509]\n",
      "Step [288/362, Loss: 1.130598783493042]\n",
      "Step [289/362, Loss: 1.0865156650543213]\n",
      "Step [290/362, Loss: 1.0901236534118652]\n",
      "Step [291/362, Loss: 1.1554826498031616]\n",
      "Step [292/362, Loss: 1.1469862461090088]\n",
      "Step [293/362, Loss: 1.283133864402771]\n",
      "Step [294/362, Loss: 1.2695633172988892]\n",
      "Step [295/362, Loss: 1.1799973249435425]\n",
      "Step [296/362, Loss: 1.1053664684295654]\n",
      "Step [297/362, Loss: 1.1698517799377441]\n",
      "Step [298/362, Loss: 1.0448522567749023]\n",
      "Step [299/362, Loss: 1.2198357582092285]\n",
      "Step [300/362, Loss: 1.1472314596176147]\n",
      "Step [301/362, Loss: 1.1065540313720703]\n",
      "Step [302/362, Loss: 1.1919100284576416]\n",
      "Step [303/362, Loss: 1.0463347434997559]\n",
      "Step [304/362, Loss: 1.0433976650238037]\n",
      "Step [305/362, Loss: 1.126830816268921]\n",
      "Step [306/362, Loss: 1.1714529991149902]\n",
      "Step [307/362, Loss: 1.2254512310028076]\n",
      "Step [308/362, Loss: 1.0940697193145752]\n",
      "Step [309/362, Loss: 1.0547338724136353]\n",
      "Step [310/362, Loss: 1.1435304880142212]\n",
      "Step [311/362, Loss: 1.0535907745361328]\n",
      "Step [312/362, Loss: 0.9829987287521362]\n",
      "Step [313/362, Loss: 1.0257917642593384]\n",
      "Step [314/362, Loss: 1.0972007513046265]\n",
      "Step [315/362, Loss: 1.0488067865371704]\n",
      "Step [316/362, Loss: 1.1077117919921875]\n",
      "Step [317/362, Loss: 1.0912432670593262]\n",
      "Step [318/362, Loss: 1.0734200477600098]\n",
      "Step [319/362, Loss: 1.113457202911377]\n",
      "Step [320/362, Loss: 1.0548999309539795]\n",
      "Step [321/362, Loss: 1.0392489433288574]\n",
      "Step [322/362, Loss: 1.0902451276779175]\n",
      "Step [323/362, Loss: 1.0942401885986328]\n",
      "Step [324/362, Loss: 1.2025336027145386]\n",
      "Step [325/362, Loss: 1.2021293640136719]\n",
      "Step [326/362, Loss: 1.1156038045883179]\n",
      "Step [327/362, Loss: 1.1330052614212036]\n",
      "Step [328/362, Loss: 1.1598527431488037]\n",
      "Step [329/362, Loss: 1.1779276132583618]\n",
      "Step [330/362, Loss: 1.119463324546814]\n",
      "Step [331/362, Loss: 1.1591130495071411]\n",
      "Step [332/362, Loss: 1.2745898962020874]\n",
      "Step [333/362, Loss: 1.1200900077819824]\n",
      "Step [334/362, Loss: 1.0510919094085693]\n",
      "Step [335/362, Loss: 1.0986782312393188]\n",
      "Step [336/362, Loss: 1.0971604585647583]\n",
      "Step [337/362, Loss: 1.0037529468536377]\n",
      "Step [338/362, Loss: 1.3400546312332153]\n",
      "Step [339/362, Loss: 1.2426337003707886]\n",
      "Step [340/362, Loss: 1.1449133157730103]\n",
      "Step [341/362, Loss: 1.1535468101501465]\n",
      "Step [342/362, Loss: 1.2896276712417603]\n",
      "Step [343/362, Loss: 1.1092982292175293]\n",
      "Step [344/362, Loss: 1.1168988943099976]\n",
      "Step [345/362, Loss: 1.1479706764221191]\n",
      "Step [346/362, Loss: 1.1408942937850952]\n",
      "Step [347/362, Loss: 1.26715087890625]\n",
      "Step [348/362, Loss: 1.0939604043960571]\n",
      "Step [349/362, Loss: 1.152802586555481]\n",
      "Step [350/362, Loss: 1.0741490125656128]\n",
      "Step [351/362, Loss: 0.9876605868339539]\n",
      "Step [352/362, Loss: 1.0439114570617676]\n",
      "Step [353/362, Loss: 1.0558500289916992]\n",
      "Step [354/362, Loss: 1.2292684316635132]\n",
      "Step [355/362, Loss: 1.1879315376281738]\n",
      "Step [356/362, Loss: 1.0350143909454346]\n",
      "Step [357/362, Loss: 1.0724836587905884]\n",
      "Step [358/362, Loss: 1.1380038261413574]\n",
      "Step [359/362, Loss: 1.0484834909439087]\n",
      "Step [360/362, Loss: 1.0439307689666748]\n",
      "Step [361/362, Loss: 1.1496562957763672]\n",
      "Validation Loss: 1.1509664058685303, Test Time: 2152.837208032608\n"
     ]
    }
   ],
   "source": [
    "# Run test set\n",
    "model.eval()\n",
    "test_loss_list=[]\n",
    "test_batch_size=2000\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range((len(data.test_text)-len(tokenizer.vocab_list)-1)//test_batch_size):\n",
    "        x, y = data.get_test_sample(i*test_batch_size, test_batch_size)\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        outputs, loss = model(x, y)\n",
    "        test_loss_list.append(loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f'Step [{i}/{(len(data.test_text)-len(tokenizer.vocab_list))//test_batch_size}, Loss: {loss.item()}]')\n",
    "val_loss=torch.tensor(test_loss_list).mean().item()\n",
    "print(f'Validation Loss: {val_loss}, Test Time: {time.time() - start}')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcH9JREFUeJzt3Qd0FFUXB/BLeiEdQhIIJZTQe2/SBAEpFppKBxVRQQQF/Wg2EFFARVCqdAHp0nvvvfceOklIb/Od+5LZzMyWbEKS3ST/3zlrtszOzuyG7PW+++7LJ0mSRAAAAACgY5N6FQAAAAAYAiQAAAAADQRIAAAAABoIkAAAAAA0ECABAAAAaCBAAgAAANBAgAQAAACggQAJAAAAQAMBEgAAAIAGAiQAAAAADQRIADnAH3/8Qfny5aM6depY+lCsTvHixen111+nvO7x48c0aNAgKlu2LDk7O5Ovry/Vrl2bvvzyS4qIiNBtt2jRIpo8ebJFjxUgJ8iHtdgArF+DBg3o/v37dPPmTbpy5QqVKlXK0odkVQFSxYoVad26dZRXPXv2jKpVq0bh4eHUp08fESQ9ffqUTp8+Ld4X/snvE+Ng8uzZs+J3CQCMszPxGABYgRs3btD+/ftpxYoV9MEHH9DChQtp9OjR2XoMSUlJFBcXR05OTtn6umCeWbNm0e3bt2nfvn1Uv3591WMcNDk4OFjs2AByKgyxAVg5Doi8vLyobdu29Pbbb4vbsvj4ePL29qbevXvrPY+/GDmgGTp0qO6+2NhYEVxxBsrR0ZECAwPpiy++EPcr8XDexx9/LF6rQoUKYtuNGzeKxyZOnCi+hH18fMRQTo0aNWj58uV6rx8dHU2ffvopFShQgNzc3Kh9+/Z07949se8xY8aotuX7OfNRqFAh8Vr8mrNnz6bMkpCQQN9++y2VLFlS7J+zKV999ZXeeR89epRatWoljpnPrUSJEuK4lJYsWSLOmc/J3d2dKlWqRFOmTDH62un9jH777Tdx/i4uLuJzr1mzphgWM+XatWtka2tLdevW1XuMj1EObJs0aUL//fcf3bp1S3wOfJEzSxn9/QgODhb75/dk9+7dJo8TIEfhITYAsF5ly5aV+vbtK67v3r2bh8Slw4cP6x7v06eP5OnpKcXGxqqe9/fff4ttjxw5Im4nJiZKLVu2lFxcXKTBgwdLf/75p/Txxx9LdnZ2UocOHVTP5eeVK1dOKliwoDR27Fhp6tSp0okTJ8RjRYoUkT766CPp999/l3755Repdu3aYvt169ap9tG5c2dxf/fu3cXz+XaVKlXEfaNHj9Zt9+DBA7HPwMBA6ZtvvpGmTZsmtW/fXmw3adKkNN+fYsWKSW3btjW5Tc+ePcX+3n77bXEsPXr0ELc7duyo2+bhw4eSl5eXVKZMGemnn36SZsyYIX399dfifZBt3rxZPK958+ZiP3zh97BTp04mX9/cz+ivv/7SHSd/PlOmTBGf/aeffmpy/z/88IN43ty5c01ux8dftWpVqUCBAtL8+fPFZeXKlRn6/ahYsaLYD39mP/74o/gcnJ2dpTNnzpg8BoCcAgESgBU7evSo+DLasmWLuJ2UlCSCiUGDBum22bRpk9hm7dq1que2adNGCgoK0t3mL0MbGxtpz549qu2mT58unr9v3z7dfXybtz137pzeMUVFRalux8XFiS/LZs2a6e47duyY2Ad/0Sr16tVLL0DiAMDf31968uSJatuuXbtKHh4eeq+X3gDp5MmT4jX79eunun/o0KHi/u3bt4vbHCgogxVD+H13d3eXEhISpPQw9zPiQKRChQpSenGQycEsvwYH1B9++KG0aNEiKTQ0VG9bfq/4PdNK7+8HX/j3U3br1i3JyclJeuONN9J9/ADWCENsAFaMhzB42Klp06a6oY0uXbqIYZ7ExERxX7NmzcSQ0D///KN73vPnz2nLli1iW9myZcuoXLlyooD3yZMnugs/n+3YsUP12q+88gqVL19e75h46En5OmFhYdSoUSM6fvy47n55OO6jjz5SPfeTTz5R3ebv2n///ZfatWsnriuPi4e6eN/K/WbE+vXrxc8hQ4ao7v/888/FTx5yYp6enuInFzXzsJghvE1kZKR4b9PD3M+I93/37l06cuRIuvbPvyOnTp2iDz/8UOx3+vTp9M4774iZbDy0aM5cnPT+ftSrV08Mq8mKFi1KHTp0oE2bNul+NwFyNEtHaABgGGcpOLPCmZQrV67oLkuXLhX/985ZCdkHH3wgubm5STExMeL2zJkzxTacPZHxUJH8f/6GLsphHL7Nw0KGcBakTp06kqOjo+r5+fKJSbHC+++/L7IR8fHxqueGhYWpMkg8rGXqmPiyYsWKl8og8XvDx8KZLi0e9uLhLDk799Zbb4nX5CwRD/PNnj1b957Kxyu/j4ULF5Z69+4tbdiwweTxpeczOn/+vNgv31+qVCkxlLl3714pPfg8Ll26JP3666+6ffFwYVoZpPT+fvAwpdbIkSPFYyEhIek6ZgBrhFlsAFZq+/btFBISIrJFfDGUXWrZsqW43rVrV/rzzz9pw4YN1LFjR1q6dKnIBFSpUkU1E40Lin/55ReDr8cFucYyRbI9e/aIYuvGjRuL3kz+/v5kb29Pc+bMSbOQ2BA+Jvbee+9Rz549DW5TuXJlygycfUvrcS42P3jwIK1du1ZkQrhA++effxb35c+fX2RkTp48KR7j95ovfO49evSgv//+2+T+zfmMOINz6dIlkcXiLBxn1/h9HjVqFI0dO9bs8yxTpoy4cGF/6dKlxe9Kv379TD4vvb8fALmepSM0ADBeWOzr6ystW7ZM79KtWzeRjZDrc7jAVs42PX78WBTWKut85HoXzihwhiEt/Kdh4MCBBmtwuBBXmVVh77zzjniO7Pvvvxe3L1++rNpOrk2Sj42zZHwefD4ZlVYGSS5g5uyMtm6H7//888+NPnfhwoV6GRglft85M8TbcHbPFHM+Iy0u6uZzs7W1laKjo6WM8PHxkYKDg3W3X3/9dYMZpPT+ftSrV0/v/i5duogi7/TWaAFYI9QgAVghniLPfY+4qR9P7ddeeIr1ixcvaM2aNWJ7GxsbcT9nPubPny+mtStrW1jnzp3FdPoZM2YYfD2urUkLTyXnDIWyxoQbDq5atUq1HdcPMc5+KPEUdu3+3nrrLZEp4eaFhrpDv6w2bdqIn9ru0XKmhLMsjGt3tLU6VatWFT/lae7cfFGJ33c5w6WdCq9lzmek3T/3L+I6MD4uY3VR7NChQwY/v8OHD4t98lR8maurq6jt0krv78eBAwdU9WF37tyh1atXi6wmf64AOR06aQNYIS7m5SEZDjy48NXQcIifn5/oeyMHSdwksGHDhqI/D/e24e7J2udwMTQP8fAXM3fn5kDn4sWLYriHh4245w7jIGjgwIH0+++/6w37NW/eXBRlcxHwo0ePaOrUqeJY+PWUf044GODAp3v37uI4d+3aRZcvXxZDVNwHSW52+fDhQ7GECgdD/fv3FwEBd4bmL9+tW7eK66bwuXIfHh6m0+Lu0hwA9erVSwyBcRDAxeccOPBtHupauXKlLoDigO6NN94Q/ZI4AOVggYc5+Zi5JxI/xsfDhctFihQR/YQ46ONjOHbsmAiCTEnrM+KiZ34v+bPhwusLFy6Iz4CDDvlzNkTuScTHx/vgwIqfy72kOHDbuXOnbpman376SfQ2+uyzz6hWrVpi6JB/L9L7+8Hdyx88eCB6XXHPJH7v+LPkYC2zhkUBLMrSKSwA0NeuXTsxZToyMtLoNjxl3t7eXjc9nodGuJcQ/7P+7rvvDD6HC5W5Zw1PJecia+77U6NGDdHriAuo0xpiY7NmzZJKly4tns9TyufMmSOGirR/TvjYeR/e3t5S/vz5Rc8hLh7m7caPH6/aloufeVs+fj4nPz8/0WuI+wKlhYeLjBUWy/2juFicz7FEiRJi//w6I0aMUA0VHj9+XAz1FS1aVJwbD2/ycJRyKvvy5ctFryB+zMHBQWzLQ2zmFiWn9Rlx76HGjRuLYTE+hpIlS0rDhg1TfTaGnD59WmxXvXp18X7z8B0P53F/Jj4vpYiICDEkygXqfBzK4bb0/n4sWLBA97tQrVo1aceOHWa9DwA5ATJIAJBtOBPDWZ0FCxbQu+++a+nDgQwylmEEyE1QgwQAWYLrVrR4GIuHoXgWHACANcM0fwDIEhMmTBB1Odzk0s7OTjct/v3338eUcQCwegiQACBL8IK23CmaOzlHRESITstcnP31119b+tAAANKEGiQAAAAADdQgAQAAAGggQAIAAADQQA2SAdww7f79+6KZW1rrNwEAAIB14KohbvIaEBCQZuPWtCBAMoCDI8yyAQAAyJl46Rvudv8yECAZwJkj+Q12d3e39OEAAACAGcLDw0WCQ/4efxkIkAyQh9U4OEKABAAAkLNkRnkMirQBAAAANBAgAQAAAGggQAIAAADQQIAEAAAAoIEACQAAAEADARIAAACABgIkAAAAAA0ESAAAAAAaCJAAAAAANBAgAQAAAGggQAIAAADQQIAEAAAAoIHFarPRi5h4CouOJxcHO/J2dbD04QAAAIARyCBlo3kHblHDH3fQjxsuWvpQAAAAwAQESNkoX77kn4mSZOlDAQAAABMQIGUj25QIKSkJARIAAIA1Q4CUjWxtUgIkZJAAAACsGgKkbJQvJYOUiPgIAADAqiFAyka2KTVIyCABAABYNwRI2chGHmJDDRIAAIBVQ4CUjWzkIm1kkAAAAKwaAiQLFGknJln6SAAAAMAUBEjZKCU+QgYJAADAyiFAykYYYgMAAMgZECBZIEBKRJE2AACAVUOAZIEaJCSQAAAArBsCJAtM80cGCQAAwLohQLJAkTYWqwUAALBuCJAssFithAAJAADAqiFAssRabBhiAwAAsGoIkCxQpI34CAAAwLpZNEAaN24c1apVi9zc3MjX15c6duxIly5dMvmcJk2aiEyM9tK2bVvdNr169dJ7/LXXXiNLQ6NIAACAnMHOki++a9cuGjhwoAiSEhIS6KuvvqKWLVvS+fPnydXV1eBzVqxYQXFxcbrbT58+pSpVqlCnTp1U23FANGfOHN1tR0dHsprFahEgAQAAWDWLBkgbN25U3Z47d67IJB07dowaN25s8Dne3t6q20uWLCEXFxe9AIkDIj8/P7LGIm2sxQYAAGDdrKoGKSwszGAQZMqsWbOoa9euehmnnTt3imArODiYBgwYIDJNxsTGxlJ4eLjqkqVLjaAICQAAwKpZTYCUlJREgwcPpgYNGlDFihXNes7hw4fp7Nmz1K9fP73htXnz5tG2bdvoxx9/FEN5rVu3psTERKO1UB4eHrpLYGAgZQWblHcbQ2wAAADWzaJDbEpci8TBzt69e9OVPapUqRLVrl1bdT9nlGT8eOXKlalkyZIiq9S8eXO9/YwYMYKGDBmiu80ZpKwIknRrsSFAAgAAsGpWkUH6+OOPad26dbRjxw4qUqSIWc+JjIwU9Ud9+/ZNc9ugoCAqUKAAXb161eDjXK/k7u6uumQFrMUGAACQM1g0g8QdpT/55BNauXKlyO6UKFHC7OcuW7ZM1A699957aW579+5dUYPk7+9PlqTLIKEGCQAAwKrZWHpYbcGCBbRo0SLRC+nBgwfiEh0drdumR48eYgjM0PAa903y8fFR3R8REUHDhg2jgwcP0s2bN0UdUocOHahUqVLUqlUrsiT0QQIAAMgZLJpBmjZtmq75oxL3L+Jmj+z27dtkI1c3p+BmklyrtHnzZr192tra0unTp+nvv/+m0NBQCggIEL2Vvv32W4v3QtJ10kYGCQAAwKpZfIgtLTz0psVT940919nZmTZt2kTWCEXaAAAAOYNVFGnnFbo+SIiPAAAArBoCpGyEITYAAICcAQFSNkKRNgAAQM6AAMkCi9Vimj8AAIB1Q4CUjVCDBAAAkDMgQMpGtroACRESAACANUOAlI3kdk4YYgMAALBuCJAsMMSGBBIAAIB1Q4BkgWn+aBQJAABg3RAgZaOUBBJqkAAAAKwcAiQLFGlzfGTOMisAAABgGQiQLDDExlCoDQAAYL0QIGWjfPIYG3ohAQAAWDUESBbKIKEOCQAAwHohQMpGivgIARIAAIAVQ4BkgT5IDDVIAAAA1gsBksWG2Cx6KAAAAGACAiQLZZCSECEBAABYLQRIFqpBQjdtAAAA64UAKZun+aObNgAAgPVDgGShbtpJSZY+EgAAADAGAVI2s0kZZ0MGCQAAwHohQLJQHRKm+QMAAFgvBEiWGmJDBgkAAMBqIUCy0FR/JJAAAACsFwIkC9UgYYgNAADAeiFAslA3bQlDbAAAAFYLAZKlirQRIAEAAFgtBEgWqkHCEBsAAID1QoCUzZzsbcXPmHh0igQAALBWCJCymXNKgBQdl2jpQwEAAAAjECBlM2eH5AApKi7B0ocCAAAARiBAymYuKQFSdDwySAAAANYKAVI2wxAbAACA9UOAZLEhNgRIAAAA1goBUjbDEBsAAID1Q4CUzTDEBgAAYP0sGiCNGzeOatWqRW5ubuTr60sdO3akS5cumXzO3LlzKV++fKqLk5OTahtexmPUqFHk7+9Pzs7O1KJFC7py5QpZA2cHO/ETQ2wAAADWy6IB0q5du2jgwIF08OBB2rJlC8XHx1PLli0pMjLS5PPc3d0pJCREd7l165bq8QkTJtCvv/5K06dPp0OHDpGrqyu1atWKYmJiyNIwxAYAAGD9ktMZFrJx40a97BBnko4dO0aNGzc2+jzOGvn5+Rl8jLNHkydPpv/973/UoUMHcd+8efOoUKFCtGrVKuratStZxxAb+iABAABYK6uqQQoLCxM/vb29TW4XERFBxYoVo8DAQBEEnTt3TvfYjRs36MGDB2JYTebh4UF16tShAwcOkKVhFhsAAID1s5oAKSkpiQYPHkwNGjSgihUrGt0uODiYZs+eTatXr6YFCxaI59WvX5/u3r0rHufgiHHGSIlvy49pxcbGUnh4uOqS5RkkDLEBAABYLYsOsSlxLdLZs2dp7969JrerV6+euMg4OCpXrhz9+eef9O2332a4WHzs2LGUHfI7Jb/l4dHx2fJ6AAAAkEMzSB9//DGtW7eOduzYQUWKFEnXc+3t7alatWp09epVcVuuTXr48KFqO75trG5pxIgRYnhPvty5c4eySqCXi/h561lUlr0GAAAA5OAAiQuqOThauXIlbd++nUqUKJHufSQmJtKZM2fElH7G++BAaNu2bbpteMiMZ7MpM09Kjo6OYmac8pJVihdIDpBCo+LpeWRclr0OAAAA5NAhNh5WW7Rokagn4l5Ico0QF1Vz/yLWo0cPKly4sBgGY9988w3VrVuXSpUqRaGhofTTTz+Jaf79+vXTzXDjWqbvvvuOSpcuLQKmkSNHUkBAgOizZGkuDnbk5+5ED8Jj6MbTSPJydbD0IQEAAIA1BUjTpk0TP5s0aaK6f86cOdSrVy9x/fbt22Rjk5roev78OfXv318EU15eXlSjRg3av38/lS9fXrfNF198IXopvf/++yKIatiwoWgpoG0oaSmcReIA6eaTSKpe1MvShwMAAAAa+SQe5wIVHpLjLBbXI2XFcNuIFadp8eE79GmzUjSkZXCm7x8AACAvCs/E72+rKNLOa0oUcBU/bzxFoTYAAIA1QoBkAcV9UgKkJxGWPhQAAAAwAAGSBRRPySDdRgYJAADAKiFAsgB/j+Ri8fCYBIqMxZpsAAAA1gYBkgW4OdmTm2PyBMKQsBhLHw4AAABoIECyEL+ULFJIWLSlDwUAAAA0ECBZiL9nciNMZJAAAACsDwIkC/FzdxQ/HyJAAgAAsDoIkCzEw9le/AyPibf0oQAAAIAGAiQLFmqzFzGYxQYAAGBtECBZiJtT8iw2BEgAAADWBwGShTNIGGIDAACwPgiQLAQZJAAAAOuFAMniARIySAAAANYGAZKFuKNIGwAAwGohQLIQDLEBAABYLwRIFi7Sjo5PpPjEJEsfDgAAACggQLJwBolFIIsEAABgVRAgWYi9rQ052Se//RGxCJAAAACsCQIkC0IvJAAAAOuEAMmCUKgNAABgnRAgWRDWYwMAALBOCJAsyB3NIgEAAKwSAiQLwhAbAACAdUKAZEFujvIQGzJIAAAA1gQBkhVkkCZuvkyJSZKlDwcAAABSIECyoEQpNSg6cy/MoscCAAAAqRAgWVDjMgV11zHMBgAAYD0QIFlQ02BfKu7jIq5HxiZa+nAAAAAgBQIkCyvq4yp+RsVhJhsAAIC1QIBkYa4OtuJnJNZjAwAAsBoIkCzMxSF5JltkHIbYAAAArAUCJAvL74gMEgAAgLVBgGRhLo4pGSQUaQMAAFgNBEgWhhokAAAA64MAycJc5QwSZrEBAABYDQRIFuYqF2kjgwQAAGA1ECBZTQYJNUgAAADWwqIB0rhx46hWrVrk5uZGvr6+1LFjR7p06ZLJ58yYMYMaNWpEXl5e4tKiRQs6fPiwaptevXpRvnz5VJfXXnuNrJFLSg0SGkUCAABYD4sGSLt27aKBAwfSwYMHacuWLRQfH08tW7akyMhIo8/ZuXMndevWjXbs2EEHDhygwMBA8Zx79+6ptuOAKCQkRHdZvHgxWSMn++QAKSY+ydKHAgAAACmSx3csZOPGjarbc+fOFZmkY8eOUePGjQ0+Z+HCharbM2fOpH///Ze2bdtGPXr00N3v6OhIfn5+ZO2c7JNj1Jh4DLEBAABYC6uqQQoLCxM/vb29zX5OVFSUyDxpn8OZJg62goODacCAAfT06VOj+4iNjaXw8HDVJbs4pwyxIUACAACwHlYTICUlJdHgwYOpQYMGVLFiRbOf9+WXX1JAQICoRVIOr82bN09klX788UcxlNe6dWtKTEw0Wgvl4eGhu/CwXXZxsksOkKJRpA0AAGA1LDrEpsS1SGfPnqW9e/ea/Zzx48fTkiVLRLbIyclJd3/Xrl111ytVqkSVK1emkiVLiu2aN2+ut58RI0bQkCFDdLc5g5RdQZIug5SQRJIkiYJyAAAAsCyryCB9/PHHtG7dOlF4XaRIEbOeM3HiRBEgbd68WQRApgQFBVGBAgXo6tWrBh/neiV3d3fVJbszSIlJEsUnStn2ugAAAGClARJnTDg4WrlyJW3fvp1KlChh1vMmTJhA3377rSjyrlmzZprb3717V9Qg+fv7k7Vxckj9CGISMMwGAABAeT1A4mG1BQsW0KJFi0QvpAcPHohLdHS0bhuemcZDYDKuKRo5ciTNnj2bihcvrntORESEeJx/Dhs2TLQOuHnzpqhD6tChA5UqVYpatWpF1sbB1oZsUkbVYlCHBAAAkDsCJC58PnnyJD1//jzdz502bZqYudakSROR3ZEv//zzj26b27dviz5GyufExcXR22+/rXoOD7kxW1tbOn36NLVv357KlClDffv2pRo1atCePXvEUJq14ZojuRdSNGayAQAA5MwibZ5pxoXPHHhwcPTKK6/Q/v37ycXFRdQRcbCTniG2tHBhtRJnhUxxdnamTZs2UU7ibG9LUXGJaBYJAACQUzNIy5cvpypVqojra9eupRs3btDFixfps88+o6+//jorjjHXQwYJAAAghwdIT5480XWoXr9+PXXq1EkMZfXp04fOnDmTFceY66GbNgAAQA4PkAoVKkTnz58Xw2s8i+zVV1/VdbTm+h9IP2SQAAAAcngNUu/evalz586iMJoLjOUO1ocOHaKyZctmxTHmelyDxGIRIAEAAOTMAGnMmDFiKZA7d+6I4TV5Zhhnj4YPH54Vx5jryd20I2MRIAEAAOTYpUZ4ir1SaGgo9ezZM7OOKc9xd7YXP8Nj4i19KAAAAJCRGiRu1KjsU8TDbT4+PmKJEO4/BOnnmRIghUYhQAIAAMiRAdL06dN1C7lu2bJFXDZs2ECvvfYaDR06NCuOMdfzdJEDpDhLHwoAAABkZIiNl/WQAyRuDMkZpJYtW4plP+rUqZMVx5jreTo7iJ9/H7hFLSv4UYNSBSx9SAAAAHlaujNIXl5eokCb8TR/eRYbd8Xmqf+Qfh4pGSQ2bNkpix4LAAAAZCCD9Oabb9I777xDpUuXpqdPn1Lr1q3F/SdOnBALwkLGa5CYvZ1F1w8GAACAjARIkyZNEsNpnEWaMGEC5c+fX9zPC8p+9NFHWXGMuZ6bU2qA5O/hZNFjAQAAgAwESPb29gaLsXktNsgYV8fUDuTersn1SAAAAJDD+iBdu3aNJk+eTBcuXBC3y5cvT4MHD6agoKDMPr48oVJhDyqQ34GeRMRRTHySpQ8HAAAgz0t3wcumTZtEQHT48GGqXLmyuPAyI3wfT/mH9OMlW0a1qyCuR8UlWPpwAAAA8rx0Z5B4OREeThs/frze/V9++aVu8VpIHxd5wdo4zAQEAADIcRkkHlbr27ev3v19+vSh8+fPZ9Zx5TkuKeuxRSFAAgAAyHkBUsGCBenkyZN69/N9vr6+mXVceXbBWgRIAAAAOXCIrX///vT+++/T9evXqX79+uK+ffv2iTXahgwZkhXHmCe4OCR/FNHxCJAAAAByXIA0cuRIcnNzo59//plGjBgh7gsICKAxY8bQoEGDsuIY89gQG4q0AQAActwQG8+44iLtu3fvUlhYmLjwdc4s7d+/P2uOMg8NsfE0/6QkydKHAwAAkKdlqA+SjDNJsitXrlCjRo2wHttLZpDkYTZXx5f6aAAAAOAlYOEvK+FkZ0v58iVfj4zFMBsAAIAlIUCyEjY2+cgjZdHa0Oh4Sx8OAABAnoYAyYp4uSSvw/Y8Ms7ShwIAAJCnmV3osmbNGpOP37hxIzOOJ0/zdEnOID2PQgYJAAAgRwRIHTt2NGuGG7x8Bik0ChkkAACAHBEgJSVhlfms5plSg4QMEgAAgGWhBsmKeMoZpGhkkAAAACwJAZIV8UqpQQqNRAYJAADAkhAgWRGPlAApDNP8AQAALAoBkhUuWBuFBWsBAAAsCgGSFS43Eo0FawEAAHJegBQaGkozZ86kESNG0LNnz8R9x48fp3v37mX28eXJACkqDhkkAAAAS0r3iqinT5+mFi1akIeHB928eZP69+9P3t7etGLFCrp9+zbNmzcva440Lw2xIUACAADIWRmkIUOGUK9evejKlSvk5OSku79Nmza0e/fuzD6+PJpBwhAbAABAjgqQjhw5Qh988IHe/YULF6YHDx5k1nHl7QApFhkkAACAHBUgOTo6Unh4uN79ly9fpoIFC6ZrX+PGjaNatWqRm5sb+fr6iuVMLl26lObzli1bRmXLlhUZrEqVKtH69etVj0uSRKNGjSJ/f39ydnYWQ4Kc8cpJs9j4HAAAACCHBEjt27enb775huLj43Xrr3Ht0ZdffklvvfVWuva1a9cuGjhwIB08eJC2bNki9tmyZUuKjIw0+pz9+/dTt27dqG/fvnTixAkRVPHl7Nmzum0mTJhAv/76K02fPp0OHTpErq6u1KpVK4qJiSFr5uKYnEFKTJIoNgFLuwAAAFhKPimdqYqwsDB6++236ejRo/TixQsKCAgQQ2v16tUTmRwORjLq8ePHIpPEgVPjxo0NbtOlSxcRQK1bt053X926dalq1aoiIOLT4WP6/PPPaejQobpjLlSoEM2dO5e6du2a5nFwhoyL0Pl57u7ulF0SEpOo1NcbxPUTI18lL9fkpUcAAACAsvX7O92z2PiFOduzd+9eMaMtIiKCqlevLoaxXhafEONZccYcOHBAFIorcXZo1apV4vqNGzdEwKY8Hj7mOnXqiOeaEyBZip2tDTnY2VBcQpIYZvOy9AEBAADkUekOkGQNGzYUl8ySlJREgwcPpgYNGlDFihWNbsfBD2eDlPi2XCAu/zS1jVZsbKy4yAzVWGUX23z5xM+D157SWzWKWOw4AAAA8rJ0B0hc22MI1yJx0XSpUqXE8JitbXI9jbm4FonriDgzld24WHzs2LFkDaJTlhn5fNkpBEgAAAA5JUCaNGmSqBWKiooiL6/kQaDnz5+Ti4sL5c+fnx49ekRBQUG0Y8cOCgwMNGufH3/8sagp4j5KRYqYDgr8/Pzo4cOHqvv4Nt8vPy7fx7PYlNtwnZIh3BFcOWzHGSRzjz2zNSvrS9svPtL1Q5JntgEAAIAVz2L74YcfxNR8njb/9OlTceEp/lzjM2XKFDGjjYOUzz77LM19cUE1B0crV66k7du3U4kSJdJ8DheDb9u2TXUf10Tx/Yz3wa+v3IYDHp7NJm9jqHUBF3MpL5Yyu1ctKpDfUVw/f99yQ30AAAB5mpROQUFB0okTJ/TuP378uFSiRAlxfd++fZKfn1+a+xowYIDk4eEh7dy5UwoJCdFdoqKidNt0795dGj58uO4279vOzk6aOHGidOHCBWn06NGSvb29dObMGd0248ePlzw9PaXVq1dLp0+fljp06CCOLTo62qxzDAsL45l94qcl9Jx9SCr25Tpp0aFb0pMXMdLJ288tchwAAAA5SWZ+f6d7/CYkJIQSEvSXwuD75CJonmbPLQDSMm3aNPGzSZMmqvvnzJkjljNhnJGysUlNdNWvX58WLVpE//vf/+irr76i0qVLixlsysLuL774QrQCeP/998XCulxMvnHjRtXSKNbM1y05g/Q0IpZa/LKLnkfF04qP6lP1opjXBgAAkB3SHSA1bdpULDUyc+ZMqlatmriPGzYOGDCAmjVrJm6fOXPGrOEyc1ow7dy5U+++Tp06iYsxXDDOzSz5khN5u6YESJFxIjhi2y88QoAEAABgrTVIs2bNEn2KatSoIWp3+FKzZk1xHz/GuFj7559/zorjzRMK5E9uEPk0Ik53XxKWHgEAALDeDBIXQHNR9MWLF0VxNgsODhYXZZYJMs47pYP2w/DUpVGSEB8BAABkmwzPIefFYvkCmc8nZRbbjSepa9IlJmFtNgAAAKsOkO7evUtr1qwRBdRxcanDQOyXX37JrGPLs3xSMkiPXqR29w6LTq5FAgAAACsMkLi/UPv27UUzSB5m49ljN2/eFAXXvCYbZN4sNiW5WBsAAACssEibu04PHTpUzFTjafP//vsv3blzh1555RWTM8vAfAXdHMnJXv3RhEapM3UAAABgRQHShQsXqEePHuK6nZ0dRUdHi1lrPKX+xx9/zIpjzHO4TUFRbxfVfaHIIAEAAFhvgOTq6qqrO+K1zq5du6Z77MmTJ5l7dHlYUW9X1e2IWP3mnAAAAGAlNUh169alvXv3Urly5ahNmzb0+eefi+G2FStWiMcgcxT3UWeQImIQIAEAAFhtgMSz1CIiIsT1sWPHiuv//POPWPIDM9gyT5VAT9XtiLgEUQjPw28AAABgRQFSYmKimOJfuXJl3XDb9OnTs+rY8rSaxdXLinAj7ai4RHJ1zHDrKgAAAMiKGiRbW1tq2bIlPX/+PD1Pgwzw93CmtpX9qW6Qt+4+1CEBAABYaZE29z26fv161hwNqEx9pzoteb8eeTjbi9szduN9BwAAsMoA6bvvvhN9kNatW0chISEUHh6uukDmk7toz9x7Q9QhAQAAQNZKd0ELz1xj3E1bWTAsFxBznRJknci4RMqPOiQAAIAsle5v2h07dmTNkYDZ2SQESAAAAFkr3d+0vKQIWE5YVDwV9nS29GEAAADkaumuQWJ79uyh9957j+rXr0/37t0T982fP180kITMN7Z9Bd318BgsOQIAAGB1ARIvTtuqVStydnam48ePU2xsrLg/LCyMfvjhh6w4xjyvZ/3iVK2op6pgGwAAAKxsFhs3h5wxYwbZ2ydPP2cNGjQQARNkDXmqPwIkAAAAKwyQLl26RI0bN9a738PDg0JDQzPruEDD3Sk5QPp23XnqMHUfPY9MXjAYAAAArCBA8vPzo6tXr+rdz/VHQUFBmXVcYCSD9CImgU7dCaW1p+9b+pAAAAByrXQHSP3796dBgwbRoUOHRN+j+/fv08KFC0XzyAEDBmTNUQJ5uaQOZ7JD159Z7FgAAAByu3RP8x8+fDglJSVR8+bNKSoqSgy3OTo6igDpk08+yZqjBPJ1d1LdvhCCruUAAABWEyBx1ujrr7+mYcOGiaG2iIgIKl++POXPnz9rjhCEQpoAKToeHcsBAACsZohtwYIFInPk4OAgAqPatWsjOMoGhdwdVbdjE5IsdiwAAAC5XboDpM8++4x8fX3pnXfeofXr12PtNQtlkGLjE+na4wg6ehO1SAAAABYPkEJCQmjJkiViqK1z587k7+9PAwcOpP3792f6wUEqH1cH1e2YhCRq/vMuenv6ASo/aiN9t+68xY4NAACA8nqAZGdnR6+//rqYufbo0SOaNGkS3bx5k5o2bUolS5bMmqMEsrO1od4NilOt4l7idmKSpHssKi6RZu69YcGjAwAAyF1eall4FxcXsezI8+fP6datW3ThwoXMOzLQM7pdBYqJT6SyIzcafFySJJHZAwAAAAssVstF2pxBatOmDRUuXJgmT55Mb7zxBp07d+4lDwfS4mBr/CMLj07I1mMBAADIrdKdQeratSutW7dOZI+4BmnkyJFUr169rDk60GNjk08ESXGJ+rPYnkbGkoemoSQAAABkQ4Bka2tLS5cuFUNrfF3p7NmzVLFixQwcBqSHo73hAOl5FNZnAwAAsEiAxENrSi9evKDFixfTzJkz6dixY5j2nw0c7WzpBekPpz2NQIAEAABgsRoktnv3burZs6eY5j9x4kRq1qwZHTx4MFMOCkxztDP8sT2LRIAEAACQ7RmkBw8e0Ny5c2nWrFkUHh4uapBiY2Np1apVoqs2ZA8ne8MB0vOo+Gw/FgAAgDydQWrXrh0FBwfT6dOnxay1+/fv02+//Za1RwdGh9gMeRGDAAkAACBbA6QNGzZQ3759aezYsdS2bVu9Am3IPjxbzRBuFolhNgAAgGwMkPbu3SsKsmvUqEF16tSh33//nZ48efJSL851TJyZCggIEA0OeajOlF69eonttJcKFSrothkzZoze42XLlqXc5GG44QApLiGJBi48LppJHrv1XNVtGwAAALIgQKpbty7NmDFDrMX2wQcfiPXYOLBJSkqiLVu2iOApvSIjI6lKlSo0depUs7afMmWKeH35cufOHfL29qZOnTqptuOASbkdB3d5xYHrT2nY8tP01rT99Pf+m5Y+HAAAgBwpn8TrU2TQpUuXRMH2/PnzKTQ0lF599VVas2ZNxg4kXz5auXIldezY0ezncMbpzTffpBs3blCxYsV0GSS+/+TJk5RRXIDu4eFBYWFh5O7uTtam+PD/dNeNNY2UFfZ0pvfqFqMBTbBOHgAA5G7hmfj9neFp/oyLtidMmEB3794VvZCyGwdnLVq00AVHsitXrojsVlBQEL377rt0+/Ztk/vhmXj8piov1mxK16oUVMCVNg1uTEe+bkFT36ludNt7odH048aL2Xp8AAAAOd1LBUgyLtjmzE9Gs0cZwbPouHC8X79+qvu5PopbEWzcuJGmTZsmskuNGjUyOQQ4btw4EXHKl8DAQLJmHaoWpu1Dm1Cwn5tYWqSwl7OlDwkAACBXyZQAyRL+/vtv8vT01BuSa926tahJqly5slgOZf369WL4j5dHMWbEiBEiHSdfuLYpJ3FzSrud1UuMpAIAAOQ56V5qxBrwl/3s2bOpe/fu5ODgYHJbDqLKlClDV69eNbqNo6OjuORUrg5pf4wvYhPI3QkL2QIAAOTaDNKuXbtEwMN9mdISERFB165dE0ui5FZ+Hk7Uq35xqhfkQ64OhvtTYZ02AACAHJJB4uBFmdnheiGefcZT94sWLSqGvu7du0fz5s3TK87mWqOKFSvq7XPo0KGitxIXbnOd0ujRo0WNVLdu3Sg3G9M+uRdUQmISnb4XRm/+sV/1+NOIWCpRwNVCRwcAAJCzWDRAOnr0KDVt2lR3e8iQIeInL4LLhdbcw0g7A41rhP7991/RE8kQnlHHwdDTp0+pYMGC1LBhQ7GILl/PC+xsbai4j34g9AQZJAAAgJwRIDVp0sRk8TAHSVo8yywqKsroc7iBZV5nqGg7IjZBTPkP8HASPacAAAAgl9UggWn2tjbkZK/+aOcfuEkNxm+nsWvPW+y4AAAAcgoESLmUm2bG2qm7YeLnXCw/AgAAkCYESLmUOb2RAAAAwDAESLmUm6PxAAlNIwEAAExDgJRHhtiUvlh+OluPBQAAIKdBgJQHh9iWHbtLp+6E0rFbz0XxNjJKAAAAaihUyaXymxhiY+vPhNCfu6+L64XcnahlBb9sOjIAAADrhwxSLmVna7rX0c5Lj3XXT9wJzYYjAgAAyDkQIOVSzyJNd86+9jhCdz08Oj4bjggAACDnQICUSw1sWooKezpTl5qBBh9PSEqtOwqPScjGIwMAALB+CJByqcpFPGnf8GbUtGzaa9DxQrYn74SKhW6vPnpBv227QpGxCJoAACDvQpF2Lle9qFea2+y/9pQ6Tt1HnzQrRVN3XCVOLoVGx9PI18tnyzECAABYG2SQcjlfdyc6MKIZnR3bKs1tf9ueHByxIzefZf3BAQAAWClkkPIAfw/ndD/H9Bw4AACA3A0ZpDyoVYVCaW+UDyESAADkXQiQ8pAZPWrSp81K0fT3apCrg63JbREeAQBAXoYAKQ95tXwhGtIymPLlyyeum8Kz2h69iMm2YwMAALAmCJDyqLHtK9JHTUrS929UNLrNqFXnsvWYAAAArAWKtPMoDxd7+uK1spSUJNGJ26G0/NhdvW12X0ldjgQAACAvQQYpj7OxyUcTO1WhL14LJnvN+m1FvV1o4MLjtOTwbboXGk2SlNp9GwAAIDfLJ+FbT094eDh5eHhQWFgYubu7U17BnbTHbbhIs/beMPj4123KUf/GQdl+XAAAANn9/Y0MEujY2dqI7tkze9Q0+Pj36y/QgWtPqdecw3TnWVS2Hx8AAEB2QYAEerzzOxh97MMFx2jnpcfU+c8D2XpMAAAA2QkBEujxcLY3eL+Xiz2Fx8SL6yFhMahJAgCAXAsBEpgdID2PiidlTBQZl5h9BwUAAJCNECCBHncnwwGSVmhUXJYfCwAAgCUgQAI9Dnbm/Vr8dzqEwqLjKS4hSfRTkkXHJYrhtw1nQujucxRzAwBAzoNp/gbk1Wn+StceR9C959HUY/Zhs7Yv5+9O/33SkOYfvEXfrDtPbSv505pT96mwpzPtG94sy48XAAAgHNP8IauVLJifGpcpSN1qFzVr+wsh4fQiJoFGrzlHiUmSCI4YN5gEAADIaRAggUlj21egRf3qGH3c1cFWd/3TJScMboMkJQAA5DQIkCDNeqQaxb2MPn7wq+ZU0M1RXN912fDabVynBAAAkJMgQII0OdqlZom03JzsKb+j6TWPH72I1bsvJCxaVdgNAABgTRAggVleKVNQ777R7cqLn1xzZErfv4+obu++/JjqjdtOveYeQZAEAABWCQESmGVu71o05NUyuttnxrSk3g1KmBUg3XkWreqZdOJ2qC5QOnk3+ToAAIA1QYAEZsmXL5+qPxIPrcniE5OMLk0iO3LzuS5b9CJluRL2NALNJgEAwPogQAKzKWesKSUYySBVLOxBXWsFiusLD92ieuO30eAlJ+hJRGpNUmRsAkXEJtDPmy/RxQfhWXTkAAAA6WO6uhZA4a0aRWj58Xv0SukCqvuNZZB4dluwn5u4vvNS8gy3VSeT+yPJXsQm0MRNl2ju/pv02/ardHN82yw7fgAAAHMhgwRmc3Gwo9UDG9CQlsGq+0v75tdd/3dAfd31MoXcRMNJUyJiEujorWdZcLQAAAA5NEDavXs3tWvXjgICAkSNy6pVq0xuv3PnTrGd9vLgwQPVdlOnTqXixYuTk5MT1alThw4fNm+5DMiYyV2qUZtKfrRqYAMq5uOiu5+zRyUVwZMhEbHokQQAANbHogFSZGQkValSRQQ06XHp0iUKCQnRXXx9fXWP/fPPPzRkyBAaPXo0HT9+XOy/VatW9OjRoyw4A2BFfVzoj3drUNVAT1VPpOBCbuTv7kRl/dzI1iYfFVcET7JTd8Lo7D3zao+MDeUBAADk2sVqORO0cuVK6tixo8kMUtOmTen58+fk6elpcBvOGNWqVYt+//13cTspKYkCAwPpk08+oeHDh5t1LFis9uX8c+Q2cd22vI5bQmISxSYk0X9nQuiL5adNPtdYDdJ3687T4sO3af2gRlTMxzVLjhsAAHK28Ly+WG3VqlXJ39+fXn31Vdq3b5/u/ri4ODp27Bi1aNFCd5+NjY24feDAAaP7i42NFW+q8gIZ16VWUdUit3a2NuTqaEcF8juk+VxtvD5n3w1acvg2zdx7gyLjEun37Vdp07kHFK5oFaC198oT+mPnVawBBwAAeWMWGwdF06dPp5o1a4qgZubMmdSkSRM6dOgQVa9enZ48eUKJiYlUqFAh1fP49sWLF43ud9y4cTR27NhsOIO8rUD+5DXbTDl84xl5uzpQ6UJu9OhFDI1de171+LJjd8WlcZmCNK9PbYP7eG/WIfGTC8RbVfDLpKMHAIC8JEcFSMHBweIiq1+/Pl27do0mTZpE8+fPz/B+R4wYIeqWZJxB4mE5yFyFPZ3T3KbLXwd1S5vUCfI2uh134U7LraeR6TxCAACAHBggGVK7dm3au3evuF6gQAGytbWlhw8fqrbh235+xjMJjo6O4gJZy8eMDJJs1+XH4vIyMMIGAAB5qgZJ6eTJk2LojTk4OFCNGjVo27Ztuse5SJtv16tXz4JHCbIxKQvcTu5S1awht/RS1h0hPgIAgByZQYqIiKCrV6/qbt+4cUMEPN7e3lS0aFEx9HXv3j2aN2+eeHzy5MlUokQJqlChAsXExIgapO3bt9PmzZt1++Chsp49e4o6Jc4u8XO4nUDv3r0tco6g1rN+cWpbOUAUbG+7+IjWnlJ31s4oXjD308UnaOO51J5YyCABAECODJCOHj0qpu3L5DogDnDmzp0rehzdvn1bNUvt888/F0GTi4sLVa5cmbZu3araR5cuXejx48c0atQo0UCSZ7xt3LhRr3AbLNfOgZcgYeX93XUB0sbBjejUnVD68t8zZu+r+6xDVKWIJw1tFSxqkriNgFJSGhFSTHwiOdkbXl8OAADyNqvpg2RN0Acpe2w4E0IDFh4X16/90EY0k+z85wExky09uHfS6pP3aNCSk6r7328cRF+1KWfwOb9tu0KTtl6mpR/Uo5rFjReDAwBAzpHn+yBB7tAk2JeKertQvSAfERyxd1L6Jw1rFaxa621Q89JG98PF3NN2XtO7/6/d1+nrlWcoLkG/A/fPWy6LZpYjV5/LpLMBAIDcBBkkA5BByj5cO8ShkU1KgMS/js8i48SMt5JfrReP92tYQvQ96jE742vqze5Vk0r7utHsfTeoT4MS1GjCDt1jf3WvQS3RLwkAIMcLz8TvbwRIBiBAsg5XH0XQqhP3qF+jEnT9SSS9+cf+l9ofrw136eELKlMoP11+GGF0iZObTyLp5J1Q6lA1eRHl9Dhy8xmdvB0qjjm9zwUAAOv5/s7xfZAg9yrlm18UYLOKAXZUyN2RHobHZnh/HBwxbXAk4/9X4KCmycSd4raLg60us7Tz0iOasPES/dSpMlUI8DD6Gp2mJy9pU8TLmVpXSm4/AQAAOQ8CJMgRHOxs6NBXLWjjWZ7GL9GFkBc0ZduVTNs/D+W9PX2/ql6JX0MOkHrNOSJ+frTwOO0aljpr0lT2CwAAci4ESJCjvFYxOWB5raI/7bz8WLQGMKVhqQK09+qTNPd793kUnbit3ldikn5x9/PIOLOOE+PWAAA5G2axQY7lbG/617dGMS9a0K+OqC8q7uNicttJWy7r3WdoOE+ebZdmF29ESAAAORoCJMixDl7X75dUNdBTd31R/zq666V83Uzua9VJ/Y7ej17EpCtAilUMz0nIIQEA5GgIkCDH+qhJSb37/uxeg96uUYRWD2xAjnapXbJ9XB3SvX/OIE3fdY0GpjSzZDYmZqZFxSXqriODBACQs6EGCXKsj5qWotolvGnTuYe0+HDykjSF3J1oYqcqett6utirbjcqXYD2XDFdm3T1cQSN33DRaAaJh9SO3npOzva2Ynr/K2UKqpYxAQCAnAsBEuRY+R3tRDfuU3fC0tzW00WdQSrm40J70pgEZ6gDtzJAmr3vJn277rzu9hvVCuuuR8QmpHlMAABgvTDEBjkeN2VsXyVADK8Zo80g1Sxmev01Y6VG4dHx4ueVhy9UwRFbeeKekeE2icJjkp8HAAA5AwIkyPFcHe3o127VqJWJ5UI8nNUBUtOyvrR/eDOj29cN8jF4f3hMgljfbe7+myaPSZlBGr/xIlUes5n2XHlMkbEJdO1xBEXFJdCaU/cpLCXgAgAA64IhNsgTnBQtAfZ+2VQETNqgSalOCR/af+2pwccWHrpNJQu6mny9LecfUkJiklgMV66PGr3mnBgWPH03jCoEuNO5++FUv6QPcd139aJe9Gnz0mRvi/9nAQCwBgiQIE9wsE2d0Rbg4awKnGLi9WuNKhcxvpwIu/Y4Ms3XXHHini44YrHxSXT9cXK9FAdHTA7C9l19Sv+dCaEtn71ispUAAABkD/zvKuQJwX6pfZBsFAGIr5uTwe2dHVIDqoxae0rdW+lFGnVI1x9HYokSAAArgQAJ8oSCbo605bPGenVHXNxtCA+FmYNnw2lVKpycfdK2EeD6pbScumt66RQAAMgeCJAgzyhdyI0CPFOH1xjX/XA9kKxX/eI0vHVZqpgS5Mj6NSxhcJ9ftSmnuj2la1Wa07tWho/xfMrQW2abd+AmjVlzTrUcCgAAGIcACfI0Bzsb+qhJKd3tPg1K0Iev6Hfo7t84yODzXy1XiAY1L627XaaQG3maKP5Oi9wO4Oy9MGowfjutPxOie2z+wVv0yeITBvszpWXU6nNi5t2B608NLtR751lUho8ZACA3QoAEeV5sQmrPokIejrrrPeoVEz+bBhckJ8WyJUpcz1QlMDXb5OZkR3a2NmYP0WndD42mA9ee0lcrz9C90Gj6KGWZEw6KRq46K+qa9l97kuFM0JOIONVt7vjd8Mcd1GjCjgwFXgAAuRVmsUGep4w1lOu38fAZT/dvWLqAWE5Ea0Hf5MVw7WxS/z/DzTE5e8QtBAx10+b7TfU+4gV4D14/qBfEjF17Tnd7+bG7NGTpKfq1azVxbKnnIdHOS4+plG9+CvROrY3idgOyWM0SKA/CYlRF5D75UwNEAIC8DBkkyPPaVvanhqUK0IjWZVX3O9nbisc4qOGhuO2fv0ItyhXSPS5njpIUEVZ+p+T/5zDWY8k7A4vmlh25kRYfvqO7ve50CD2LjKMlR1JbCLATd0Kp99wjIhukFKPIDGnbExy+8UzV/ZuDN3MySU8iYunkHRSUA0DuhQAJ8jwOhBb0q0MfGKg9UgoqyJmZ1CJveRjNxzU16yL3MDIUIDnY2tDgFqn1SkoNShnu3G0Kh2WcWdpwJkQMu10ISS3w5gCKRccliq7dsum7rtGplMAmJCyavvj3tO6xu8+jqcrYzdTm1z0mX5eLvWt+t5U6Tt1Hx249T/dxAwDkBBhiA0iHeMVwVT5ugc3T+ot40BevBVOgV+qwlqEA6fSYliJIGrTkpN5jBTIwtPXf6eQC7jn7kpc98VKsNzdr73VxPGPXnqcyih5QrMPUfXRubCs6e089Y67bjOShPe7FxMN18vlpz1+5zAovn1KjmFe6jx0AwNohQAJIh/gEw8XRyplwhhbHZY52NgaDjowGSFrPo1Jrm6buuKa7LmeMlMZtuGC08JzxUJuniwOduRsmgr2iKf2enqdkpmSmlkZJSpJEETvPkHscESuWUwEAyCkQIAGkQ+tKfvTP0TtUooDptdj8FcuZyIwFR8wnf/prk17GgoPq+iWtxy9iRZ1Ru9/3kquDrVgM+MjN5/R6ZX/Vdva2+YwGWK9N3k2vlCkoFuXl+qbNnzUWbRDY9ccRtO/aU+pWK1DM+jMHF6vzECbWqwOA7IAACSAd+At/1cAGaQZI79QpSpO2XjZ7v5mRQcpMnPE5mLJOXGRcIvX9+6iuhklJOYNP6d9jdykkLIaWHLmjWsBXDpCa/bxL/OSC8L5GmnBqg6Pq324hP3cn2j60yUucGQCAefC/YgDpwFmgqoGeRmepKZc22fNFU2pbSZ1xMbq9IkDirt2Hv2pOHasaXgZFKaig6UBNq1vtQLO24wzS6XvJC+uakpBkeMZbjKK3lOzaY/115g4aaFxpCC/uy1mo608i0Q0cALIFAiSALMK9iLgnUXozSK6OduTr7mTW0FOzYF+zj6daUU+qG+RjdoAUEpraI8mY6LjkAOlpRCwlJiUHLu/PO0oTNl7S29ZQt26uUzKFg6Hk/aZuF2tmQ8vI2ASxxMqjF2mfBwCAFgIkgCzUtXYg8cz/NpX8dPdxBkrLW1GD5OqYXDytTJS8VsGPPmtRRnebh5omdalCNYt7m30s3OySC68NFY9rcY+jO8/TXn4kKj6Bdlx6RDW+20oTN1+iR+ExtPn8Q4Pbcg1T7zmHaali2C0hjQDpvVmHqNXk3RSnKI6PjTcvQBq+4oxYYuXzpad0Xcp5qZYTt9GaAADShhokgCzExdrnv3lNFYTM6VWL9l97KpYN2XjugbjPRdGpW+7mLSmyJtO716BtF1IDjzerF6Y3qhWhXZcfG33t/o1KiALph+Gxun5PylYAyoxVbIJ6dho3ozRHTFwi9Z5zRFyftvMalfNPXfjXkB2XHouLTM46Gdx3fCLtu5o8BKfs8cTDdx6U9np3/P6yPVeeiJ9fLD9Ne68+oS3nH9DFb1un+XwAyNuQQQLIYhyYKGewebk6iA7dyvpmR3sbvWaTWsUVheHy0ieVCyd38za0FEqzsoV0wZHcPNLTWT+DZKglgbm2X3qkur3saGp2yBwcsNT6fit1n3WI6vywlS4/fEEbz4aIdgLPo1KDtjjVcilJqr5Miw/fNrnYrhwU8muxGDMzUACQtyGDBGAhCYmSwTXg7IwFSD6pAdLTlH5EHGwdGNGMXOztqMo3m3WPc/PGOiW89YqklUN5slGvlxfT/rcqMlTmuvMsWnVbztakB9c78YW1nLRbVy/1bYeKum2UwRJnkHjdOH7PZu69LuqduIbr6P9aqOqPZMUU79vLMtZAEwByHwRIABaiHF5SZo24uaKgGX1SbqNc/03uubRhUCPR2LFTzSLEu9btJwXPAuPlUZZ9WI/Grb9Ax28nN5CsVNiDZvasScWH/2fWcR/+ujntuPiIvvz3DGWVE7dDdculsGcRqde5h1KH3/dRtGLhXe7ZpLQvJVskv2+85IrS8H9P0/i3KqfrmAYtOUHn74fTXz1qptnmAQByPgyxAVhIk+CC4qd7ygK3aWWQGBdmVy/qSR8aWDeO63861woUGQ45mHqzWmHd4792rSZ+1iruTRUCkofmWFotC7R83ZzEsGFW6zH7sMEMEme7lMGR1sUH4fT+/GOqWiZuXKmk7M9kjoTEJFp98j5deRRBo1afVe37nRkH6Zct5ve8AoCcARkkAAvpVruomFVWs7h6CQ45I9S9XjFaceIeNS6THEgxLszmi7l+eLMSvVu3GFUL9FRllGIVfYrM7WTNvulQIfn52VzHo8wmybVEWu/NPETfdawoZsspRRsIkFjfuUfof6+XNysb9ESRweKgSPbv8bui4J4vQ15NnWWYGUN5Oy89purFvNIdwBoKGDl7iKVeANIHGSQAC+HApF2VAF1ANKNHTbHobd2g5NqhakW9RMPI2T1rZvg1ONPD9Uja4TbuYG0MD8PJln5QT3e9V/3i1KNecXHd2SFzMkhDW5ZJd4BkDAdO3/13XmR7lDiYMxQgbbv4iD5felIEI/w4d//m2ibG+zh7L0w3DPogPEb13slB0u2nhovDf912hX7ZrN8LypQIRd0Uz07sPfcINflpB70MPrfXJu+hN//Yr7eOniHclyqrGnHuvfJEFOJvv5j+WjeAPBcg7d69m9q1a0cBAQFiWGDVqlUmt1+xYgW9+uqrVLBgQXJ3d6d69erRpk2bVNuMGTNG7Et5KVu2bBafCcDLe7V8IbHorbII2NyGkemlDZjYio/q0weNg2j70Ffo81fLiE7gtUt4041xbWj1wAY0ok3qv6PXKvqJjt/cCPMrxf3s3wH16O8+talKkdRhPOUwYMNSBXS3u9Yuatbx3jQSiGhtvfCIxq49r7rvXmg0/bHzqrjuognsuA6r4Y87qP/fR+nzZado9Jpz4v7JW6/Q67/tpWkpz3sQlhognbobRhVGbxJ1TnJxuTLojIpLEENuv26/SkduPhPNKrVBm9bRm8+o4uhN9HNKUHX81nPdAsSXHrygjAqPTtAr7DeGj7HNr3uo858HsiRI4p5WPKuyz9zkZWteBtehfbXyjMnZiwA5OkCKjIykKlWq0NSpU80OqDhAWr9+PR07doyaNm0qAqwTJ06otqtQoQKFhIToLnv37s2iMwDImbjpZGFPZ/q6TTndfTwEM6JNOVFj9Enz0qITOOOArUqgp2qmHS8Yy8NTW4e8Qu83Tq2H6l63GNUo5i3WrCvk7qT3uoFezjTuzUrk4+pAg1uUfqnhI22wYwoPV8kNNrU4gDp885m4vuL4PfHz9x3JgdHEzZep8/QDNHZtcuAk48zSkKUn6ZEiQJJnzvFwlqzT9AOiWeWfu6/rvS5nqBYdui2CkTEp+/9t+1VdbyrZjSeRlFHPFLVbpnpOye/DxQcvxBClMrCyRl3+Oijeu48WHrf0oUAuZtEapNatW4uLuSZPnqy6/cMPP9Dq1atp7dq1VK1acgEqs7OzIz+/1M7FAKDGwc++4c0yfb88PV9mKPgpXSi/eG2ekq+dLl+xsDudvZfaEDKtbBsHO/MP3krX8XHQxuu5maLN2MjBkxZnQ5RBIw+RcduFqNhEg00rBzYtpbqPM1Rs9cl7dOuJOhPCCwTrriuG3pTZnkWHb1P9kj5UytdNDA2GRsXrglrZs8jUAI4zW6Yol3Dh5Vk8XqI/VlaTM3dnzFgvECBPFmknJSXRixcvyNtb3e/lypUrYtjOyclJDMONGzeOihY1nsqPjY0VF1l4uHl/pAEg2bpPGtLx28+pY9XUWXODWpSm03fDyMvVnj5oXJL+OxOiCxIM9RIa2KSU6FnEwzyMu4+/W6cYzd53Q2/bXzpXET8rBLiLJUXMVcg9dc07Y17/Lfn1zXFbMcTz6RJexiS5dYKWnGm6+zxKDM11VMwuPHRDPwCLVgQzkQYCm3+O3hGZKXZzfFtq++tecSw8LKoMkp4qisvlmX8fzj8mtuV2D8pMlVx/JQd/pQu5kbUz1lRVi4dY3ZzsRYYTIE8ESBMnTqSIiAjq3Lmz7r46derQ3LlzKTg4WAyvjR07lho1akRnz54lNzfD/+A5gOLtACBjKhb2EBelIl4utHFwI11A1LSs4YV1ub6Jh5FaV/JX3c8ZKGcHw1UA/GUn1zCdvR8mpv6bo5CH/hCbVryigWd6GAuOlEXmHafuEzPieCjLGB5yU2aQlMXbhl6Lt5cDtUYTdojGoXLhv7I9AveC4mE2eXmbb9aepx/fTu0FFR6T+joPFUXpWenW00jqPusw9W1YgnrWL04LD90Sy8p8076iwTo5LVMtMWQhYdG6xZO71Qo0WNO35PBt0XaMZ5YC5PhZbIsWLRJBzdKlS8nXN/UPLw/ZderUiSpXrkytWrUS9UqhoaFiO2NGjBhBYWFhusudO+nrkQIAhskTJUzh+iZlRkV5v5NiCMsY7rit7KJtireBxXqzC898k9sFzNl30+h2HBxFKYKiZUfvqtoyMFdF/dXB6+oM1IcLjotsnrYwmzNI4YrZfJyFUq5x90IZIL3IngDpy39Pi+COi+P5/fl65VkR7J68azzYVOJauLQoW1JEGhj+5MwZZyFHrDijyqIB5MgAacmSJdSvXz8R9LRoYfoPo6enJ5UpU4auXk0ufjTE0dFRzIpTXgDAMnjYjL3fOIhuKYaw/FOyP9rGmhyA8VIjyz+sR1UDU2ugDOHtDFEuJpxV5KHDtPB0fGUGibNr/f4+Sv+lLCDMfY3+PpBae9VtxkHV80/dCRXT+rl1wZMXqQESF49r2x1wBkemDA4eKdbw0x6bOS0XzHXqTmoNkRzUyUHNzSeR4mKKvW3aGaRExYy8F7H6AZCyy7qy/QW3PFh54u5LFclDzpbjAqTFixdT7969xc+2bdumuT0PwV27do38/dXpewCwTtwiYM3HDUTH7041kptidq5ZhOb1qU2tK/rR0g9TezMp1SzurWohYIiPgbXo2IlRr+peK6tcf2zeFy1nfbRLo/AadwMXHaeq32wWfY3MMXnrZVX9FmdoeDhSidsIGMoghSqG5pQLA1f7dgtV/3aLyT5a2iFBHgLkQnSlDr/vpUlbLqs6oiuHDZ9GxlKTiTvFRfteKMnDZYdvPKO2v+4R7RK0lK0ADA1XKhdCVg6vrj19nz775xQ1nbiT8qr7odHU9a8DtCllWDavsWiAxMHLyZMnxYXduHFDXL99+7Zu6KtHjx6qYTW+/fPPP4taowcPHogLD4vJhg4dSrt27aKbN2/S/v376Y033iBbW1vq1q2bBc4QANKLszyViyRnguoE+dDBEc1p3JuVRdHwtPdqUFk/4xle5RR7Y/s2xMXBjn7qlFz4LStTKL/JffEQYFbgoR5j3cJ5ppq5tMN4HIB8vOiE0f1FKAMkA401lS0NOBPFTTK1zSf3X3si+jlx8CPbePYBDVqS/Dde2UtqyrYr6v0r6p4uhqTWaHFxvxzYhMfEq5agsU+pQeIv8XP3w6n3nCOqfXIGqpfiPuU5ymIUQ3DKocxjKb2oLIGDSg5KLNHniWc7yg1Qx649J4ZwP1As3ZOXWDRAOnr0qJieL0/RHzJkiLg+atQocZuLrOVgif3111+UkJBAAwcOFBkh+TJo0CDdNnfv3hXBEBdpc/G2j48PHTx4UDSXBICcx8/DyezZStrMR1k/N3JTDMkpM0jcB4qnyXPGypBF/euKGWJze9fSe4wLyKe/V53K+7uLnlKZhU9TWReUmVaeUGdx2I8bL9K1xxF6Q2xyZomn04/bcIEOXHtKiw/dVmUWGv+0Q1yUX+pygCIHP3yf3HwzPc1Ale/B0GWnqE/Kfmfuvk67Lyf3tGL2KUOjcounF7EJouCae0xxMMUZKCV+XEu5dIwyM2Znkzlfjxzccc+mp5oFlU3ZcemRCEq46N5cPCRorLN7enCGkj/XCyHhqkaoeZFFZ7E1adLEZMdWno2mtHPnTrPqkwAgb+rbqAStOXVfTHXnZpTclLLD1H26x71dUwMkXoNtQb86quf/2b2GCAZ4yRfOKrGimt5CjHsw8Uyx9YOSZ+lN2mp8sdrSvvnFIrfm+LhZabFMSXpxkHf7aSSNTJn6r8QBonL4TIsDkJUfNVDNYgtLCTR5Yd4NZx/Qn7vUjS7leiHeL3cZ5yD2j53XVL2U2KStV1SZJ1OuKt4jbZDIvag4wFA2vmS2BiYAmGr7oMwgcS8pbuCprGlTHr859U3mGP7vaVp3OoQ2nA2h+X3r0D9HuDmo6S7yRzXrCRpbuqWYj4uurcPXq87Q4sPJE4xm9qhJLcoXMvl8Pn+eTclBvnLGoDwjctO5B2b/j0lulaOn+QMAKFUI8BD1RLyenDx7TjnspmzsWD6lGFypVQU/cVEKKphfdBznQEP+8uWAwBwbBjWi1lPMqxniQOrTZqXo2qMIMaxkzDt1ioqMhFLj0gXonCL4U3q7RhGTs+Z46I0X+uWMi3aIjdeEM0Q5tNblrwNUML8jHdUMSfEX8PRd18hc3Mlbdl+xtIvswPWnlI/UX9jagCwtyhqkBQdv0U+bLhndn50iQOIsE69rKDfuXHXyHnWoWli1bqGhmq0vU4IjuY6M37cv/03+HWpWzlcUw/OkBOVMT87aKPtX8X60s/W4EJ+XbmGc5WRycMT6zTsqGsFyg1JuXWCoaSsvycONVgc1L02fGVhoefLWKxRkxkLOuVmOK9IGADCFeySZai3Aa85x3x3+YjBX/8ZBqv/jr1XcS/W4oZfjgIrXntP21qlTQt3YlvVuUJxm96olio5/f6eayS9eHkrZNLgxzelVi8a/WYkW9auTvOaknxu1qlBIvKbMwc6GbNJos8C45ombeiprk7gWxVhNF68zJ7v1NEovOGJvTT9gspg7vbh5Jfc00gZVOy4+MnsfnJniwO38/XAao1mzT1uDpHzflBm49+cfFe0IJmy8qFd/xffJ6+6tO31ft3SNnJEKUQR+vIQNd1Pff+2p7j4eUWn3217RgkH2xMDQ3ClFGwTlEKFSlz8P0PgNF/WWyZHJXei1tWBK19Mxg49/L0/eCdVrSZGTIYMEALkaL0uy5fxD+rZjRd2ac3zJiPl9a9O2C49EwKT0Xp1i4guHa5p4yGLm3hv0Vco6d7yY7+LDqRmfN6sX1uuePbpdBd11Dnb2j2gmZm/xMAovoqvEQU+wn5u4KHFw9Wf3miJLwbPNmKOtDRXxSm4aKWtXJYDK+bvpmicaU36UeiHw9OIsh9iPvzudz4S6qtl7b6iyTLLec9WF2abMO3BLXEy9Bmeprjx6oQpMOLtW0M1RBAH7riYHNMuP3aVvOiT/TrF3ZhzSZRd71CtOD8LUgQ3PkOO2AdqaK64Na5Ay+5LbOzzQNOnkIUy58ef2iw/1Fvu98jCCdl3WDxLvPk9+r5Q1WzLtzECe/cezQPn8jElKkkw27/xrz3URkHWsGkCTu6Yu/ZWTIUACgFyNsyz9GpYQM+JeVqPSBcVFi4OhGsW8xCK9PJzBXaHl2hDOaPF9cg8iL0Wzytcr+4tFe7XcnezFhYMpHubK72RHBVwdaNHhO6I/lCm8HhzXpnBmh4cROYPFwymy/7UtJ46Hsyjy8I+sdnFvo2vPZVQp3/wZDpCGtQqmeQduiuyRoeAos2298EhctOQGm3eeR6mK/GUjVpzWXb+QMgPPUA+nGXv0l83hAEgOPgw1qlR2NdcGR2z4itNiBp8xhuqIesxODuZkb08/IJae4WDWmIi4BIqJSxTBIq//p8SZrz9SFnhedfJ+mgESb59WA1lrgCE2AMjVfPI7ZkpwZIqzg63oBs7BCX/RaReN5aJuQ4XiXE+k/bJR4i+RX7tVox/eqERDWgbTka+biyVc0jKrZ036/o2K9Ns71UTtjLLHEw/98X2/v5M8C0/1vF416cvXyorrDil1Lxz48bBfRnGwllGdawbS568Gk6XJQ2zKWV08XMZf9By4Ket/5CEvzkKZg4c3g75aL2beGSqm333lCc3cc10321DLVHBkaDYeN/o8YqAInLOchtb9Uxa41/5hG7X4ZbcI6mRhUfFU54dtqiL/tMzYc50a/rg9XTVqloAMEgBAFpvUpapo9MjFsBxEyUzVGhli7v91c9ClDLyU6485pxQbM6574mn8PK1/UPNSIts1oElJcVH+nz4PyVQs7E5n76U/E+TrlvYCwcr3Q1lIXSC/gxjaModytuDw1mXFl/jc/caL07mpKGeEONOWFjn7xzMkZXyc/L5pa6C4iFs5G4wzTeZkv7geibvBa8kF+d/9d4EyQllszl5RtGbQDtUNWqzuV6XEzThl50PCdBMVdl15bPZMRWWfKx4CjM/EGrWsgAwSAEAW46GuHUObUPsqAar14JSz6rKSg+JLUhlk8ZfclK7VRJfyGsX0i8flbTlDtu6TRvSmZs08c5ZoKejmlOYQaFBBV92MO2WmjV/f3ACJM10y/uKVM2HGfPBKkJidaG5/La5j09YvcZ3StJ3qLMjm8w9pzJpzunX3Rr5ensxlqh1DRikX9OWhPGOvceZemJgpaMzgf1KDJ0XzcXqSgV5JF1IWay5rYkjPGiBAAgDIRsop1y6KRWezkqEV7DNEkYw49r8W9O+A+qK5pVLdIHWgxQEOBwl8rn+8W131GA/j8ezARf3qimCFMz/c6oAzR1wQz0oWzG80EBvbPrW4XblGHA9zOtmrn8Oz+3imn8zV0U5vKNSYf4/fo6kpNTZKZ+6GGcwOKTNXygCPZytqZ0Aq7b5iuK2CMeYkFJU1SIZmxGVEqKIflaG16q48fEHfrD2vN+tQbltwNWX4Ufl5WCMESAAA2Yi/vKd0rUpj2pU3+wv6ZcmZmWpFX255FDkrImd4Khb2oOvj2qoKx5VF6PIQG7dVODOmlZhRqOTpYq/LZPHMQK6NSq61aiF6WsnZq3ol9WvIapfwFsXw8iLGjcoUpI+alKTiPi6iMF07HMnZE2WwwsN5XEBuDp6+zhetq0bqgpSUwR0Hicr157S0/apGtzOdffrxrcppvv7lhxH0xh/7xFBqu9/3UmYICYvRtTO4qVjwWDZq9TmxDmCrSbtVzaC57cOH84+JGX081KudYWltUIMEAJDNuMlgduLsyf7hzYwu1msu5XppygCkkKIIXQ56tOvfcSbDVtPo0VjApg1uOBjaeemxwS7a3NeKu5+/XjlAtED4wsjQWqIkiYaWymBF24AxvbTDa1o8zCc3mEx+TTvVFPsZPWpS/3n6M9PaVvKnMe0riAybcgaislUDB0/mrs3HzUCViwG/rF+2XBZ9n9pXKSxql7TkoTou3Oagmj+qW08j6a1pB3TbcLBq7TPZkEECAMgDAjydX7rmiYe7DOEhMZmyUSUHIZwBUh1HSsaHh1e+f6OSWa+r3KfuNVOyQdwj6M3qRURwZApnkNwVw5tygby2VmlSF/WixTJuFcEzA3lWXloLGcu42F051MdZE2WApFzmROnSwxcigPBwsaef3lZniTpUDaDfulUTgac2GJUd+qo5TdA8LyPm9K5Fk7tUNfgYL2L71cozen2btHj2XaMfd6iCI+1sTmuFDBIAAJiFsxb8pfxe3WKq++XeUCULuorhrZtPokQzylYV1cu2sA2DGovZYzw8Zy5tOwIuqzG3wFrGbRiUncG5BkkOYhYdvkV3niXXy5Qq6KbKXMmzt7rXK0bFfFzp3TrF6Ivlp8TQlTmUQSkHizGKmVvKgE3pDUUxfKeagWKdOl43TjzHKfU5nkaez3VuvGTOF8tPv1R/q6bBvnoF2unV9a+DBu83du7WBBkkAAAwi6+7k8j6aDM6nA04MfJVMdONh61GtSsvvtiVX+YyzoqkJzgSr+vmKHpGVSrsIRYU5ronc9bD2zqksSgQ/6t7DREIKVscKIfXlGumuzqmbtOsbHKAIGfgZNo14TirxEvITH9PXYTOlBkknlH2c+cqYsiJj8tYkMA1W0rcu0rm7mynKr7XzvLj5/KwHgdJ79YpSt3rFqN/PqhLxvBwnnIBZ2MNO5WfxXcpXelNMZbdykmQQQIAgJem7O+U2bhWhZtlppe2H5Szg+GcAAcG8tIccmaJNQkuSPuuPqHSvm6qgOqxZjYYd1LXLnJsKIPE67txVub82NdENkm5QLCMsz3KuiXG/alk2qCTZwbO3XdTt8Cxsq2AcgizZjEvsWYeF7Ur14Sb2KmK7rk+rg5iZmCP2YdVrzHglZLUo14x1XFwkNtx6j694+dgkYv2uS5M26ldydSyJtYCARIAAOQJDUvxUOBFveU3xr9VmSZuukR9GpRQtV5wsbej+X3r6O1HnjnHNgxqpJquPu7NSjRixRnRskBepFYmv65cl+XmaCcyTzzsJq9dZ2hpEGUGSRuI1iruLYKf4O1uVLmI8czcH+9Vpw1nHojla6ZsvSLWC5Rn2XWrHSi6gXMjU15PkBdCLqWY4cczL5XBEasa6CnOjWekKfFCyhzgaZuKaof5EhEgAQAAWE/DztUDG+gNz3G3a+52znhaenAhN4qKT6AAT8PDeBxIxMQnUdfagXrDjVyDxUNzcgdx5Uwt7WKv/NiS95OHv2p8t1X0cmqhaYXAEhRBCAcwWryfT5vrr+mn5OvmJNoisJrFvXQBEh/T9x0r0ZBXg3XDdU0VQ4um8Pso127xWm58dnL2y1VTnK8tyq76ki0nsgMCJAAAyDOqBJr+YuZg479PG5JkosEmzyDjWiJjlG0PlJQL3Cpfj635uIGYGdaxaoDeNhzQJO/X0ay1+NISpJmNyEGSuR3LlXjoUQ6QOJOlxMHY9SeRtD1lKRY+dmX26ZNmpcja5ZOUXZxACA8PJw8PDwoLCyN3d+tuhQ4AANZt1+XHdONxBPVqoC6+Tg/uTs1BjKemEWdGbT73QARyaQWMptx5FkXvzTpEvesXN3pu/DpLj94RTS25zutCSDh1qRWYZT2QMvP7GwGSAQiQAAAA8vb3N6b5AwAAAGggQAIAAADQQIAEAAAAoIEACQAAAEADARIAAACABgIkAAAAAA0ESAAAAAAaCJAAAAAANBAgAQAAAGggQAIAAADQQIAEAAAAoIEACQAAAEADARIAAACABgIkAAAAAA077R1AJEmS+BkeHm7pQwEAAAAzyd/b8vf4y0CAZMCLFy/Ez8DAQEsfCgAAAGTge9zDw4NeRj4pM8KsXCYpKYnu379Pbm5ulC9fvkyPbjnwunPnDrm7u1NuhHPMPfLCeeaFc8wr55kXzjGvnGd4Bs+RQxoOjgICAsjG5uWqiJBBMoDf1CJFimTpa/AHnlt/sWU4x9wjL5xnXjjHvHKeeeEc88p5umfgHF82cyRDkTYAAACABgIkAAAAAA0ESNnM0dGRRo8eLX7mVjjH3CMvnGdeOMe8cp554Rzzynk6WsE5okgbAAAAQAMZJAAAAAANBEgAAAAAGgiQAAAAADQQIAEAAABoIEDKRlOnTqXixYuTk5MT1alThw4fPkzWYvfu3dSuXTvRfZS7h69atUr1ONfyjxo1ivz9/cnZ2ZlatGhBV65cUW3z7Nkzevfdd0VTL09PT+rbty9FRESotjl9+jQ1atRIvAfcJXXChAl6x7Js2TIqW7as2KZSpUq0fv36TDnHcePGUa1atUSHdF9fX+rYsSNdunRJtU1MTAwNHDiQfHx8KH/+/PTWW2/Rw4cPVdvcvn2b2rZtSy4uLmI/w4YNo4SEBNU2O3fupOrVq4sZGKVKlaK5c+dmy+/DtGnTqHLlyrrmavXq1aMNGzbkmvMzZPz48eJ3dvDgwbnqPMeMGSPOS3nhfxe56RzZvXv36L333hPnwX9b+N/80aNHc9XfHn7vtJ8lX/jzyy2fZWJiIo0cOZJKlCghPqeSJUvSt99+q1oTLcd9ljyLDbLekiVLJAcHB2n27NnSuXPnpP79+0uenp7Sw4cPJWuwfv166euvv5ZWrFjBv83SypUrVY+PHz9e8vDwkFatWiWdOnVKat++vVSiRAkpOjpat81rr70mValSRTp48KC0Z88eqVSpUlK3bt10j4eFhUmFChWS3n33Xens2bPS4sWLJWdnZ+nPP//UbbNv3z7J1tZWmjBhgnT+/Hnpf//7n2Rvby+dOXPmpc+xVatW0pw5c8Rrnzx5UmrTpo1UtGhRKSIiQrfNhx9+KAUGBkrbtm2Tjh49KtWtW1eqX7++7vGEhASpYsWKUosWLaQTJ06I961AgQLSiBEjdNtcv35dcnFxkYYMGSLO4bfffhPntHHjxiz/fVizZo3033//SZcvX5YuXbokffXVV+L943PODeendfjwYal48eJS5cqVpUGDBunuzw3nOXr0aKlChQpSSEiI7vL48eNcdY7Pnj2TihUrJvXq1Us6dOiQOJ5NmzZJV69ezVV/ex49eqT6HLds2SL+zu7YsSPXfJbff/+95OPjI61bt066ceOGtGzZMil//vzSlClTcuxniQApm9SuXVsaOHCg7nZiYqIUEBAgjRs3TrI22gApKSlJ8vPzk3766SfdfaGhoZKjo6P45WT8S8jPO3LkiG6bDRs2SPny5ZPu3bsnbv/xxx+Sl5eXFBsbq9vmyy+/lIKDg3W3O3fuLLVt21Z1PHXq1JE++OCDTD9P/qPFx7xr1y7dOfE/Iv6HLbtw4YLY5sCBA+I2/2GysbGRHjx4oNtm2rRpkru7u+68vvjiC/HFptSlSxcRoFni94Hf85kzZ+a683vx4oVUunRp8WXzyiuv6AKk3HKeHCDxF4UhueUc+d9/w4YNjT6eW//28O9qyZIlxfnlls+ybdu2Up8+fVT3vfnmmyKQyamfJYbYskFcXBwdO3ZMpBOV673x7QMHDpC1u3HjBj148EB1/LzWDadn5ePnn5wOrVmzpm4b3p7P89ChQ7ptGjduTA4ODrptWrVqJYa5nj9/rttG+TryNlnxPoWFhYmf3t7e4id/RvHx8arX5xRt0aJFVefJ6dpChQqpjo8XVjx37pxZ55Bdvw+c8l6yZAlFRkaKobbcdn48JMFDDtpjyU3nycMPPOwdFBQkhh14mCU3neOaNWvE34xOnTqJYaNq1arRjBkzcvXfHn5PFyxYQH369BHDbLnls6xfvz5t27aNLl++LG6fOnWK9u7dS61bt86xnyUCpGzw5MkT8WWl/OVmfJt/YaydfIymjp9/8h84JTs7OxF8KLcxtA/laxjbJrPfp6SkJFGz0qBBA6pYsaLutfkfHf8DNXWeGT0H/mMWHR2d5b8PZ86cEXUMXIfw4Ycf0sqVK6l8+fK55vwYB37Hjx8XdWVaueU8+YuDa0g2btwoasv4C4brLnil8txyjtevXxfnVrp0adq0aRMNGDCAPv30U/r7779z7d8eru8MDQ2lXr166V43N3yWw4cPp65du4rgzt7eXgS7/DeWA/uc+lnapWtrgFyCsw9nz54V/4eT2wQHB9PJkydFhmz58uXUs2dP2rVrF+UWd+7coUGDBtGWLVtEAWZuJf+fN+PCew6YihUrRkuXLhUFrrkB/48KZwt++OEHcZu/VPnf5fTp08XvbW40a9Ys8dlyZjA3Wbp0KS1cuJAWLVpEFSpUEH+DOEDi88ypnyUySNmgQIECZGtrqzcrgW/7+fmRtZOP0dTx889Hjx6pHucZFjwjQbmNoX0oX8PYNpn5Pn388ce0bt062rFjBxUpUkR1npyG5v+7M3WeGT0HnpXBX2xZ/fvA/zfKM1hq1KghMixVqlShKVOm5Jrz42EC/l3j2Tr8f5d84QDw119/Fdf5/xRzw3lqcYahTJkydPXq1VzzWfJsJs5uKpUrV043lJjb/vbcunWLtm7dSv369dPdl1s+y2HDhumySDwc2L17d/rss890Wd6c+FkiQMoG/IXFX1Y8Pqv8Pye+zbUh1o6nbfIvlvL4OW3LY8Ly8fNP/gfOX16y7du3i/Pk//OVt+F2AjzeLuMsAGc8vLy8dNsoX0feJjPeJ64/5+CIh5z42Pi8lPgz4tSw8vV5XJv/WCvPk4ewlP+I+fj4j5D8hz6tc8ju3wfed2xsbK45v+bNm4tj5P9DlS+cheBUvnw9N5ynFk91vnbtmggqcstnyUPc2lYbXMPCmbLc9LdHNmfOHDGExLVzstzyWUZFRYlaISUOyPg1cuxnma6Sbsgwnl7J1fpz584Vlfrvv/++mF6pnJVgSTwjiKeP8oV/LX755Rdx/datW7rpmXy8q1evlk6fPi116NDB4PTMatWqiem6e/fuFTOMlNMzecYCT8/s3r27mJ7J7wlPS9VOz7Szs5MmTpwoZnLwTJ7Mmmo7YMAAMcV0586dqim3UVFRum14ui1P/d++fbuYbluvXj1x0U63bdmypWgVwFNoCxYsaHC67bBhw8Q5TJ061eB026z4fRg+fLiYlcfTbPlz4ts8A2Tz5s254vyMUc5iyy3n+fnnn4vfVf4s+d8FT/Hmqd08+zK3nCO3aeB/7zxF/MqVK9LChQvF8SxYsEC3TW742yPPGOPPi2dcaeWGz7Jnz55S4cKFddP8uWUM/77y7Lqc+lkiQMpG3JeC/xFwHwqebsl9HqwF9+PgwEh74V96eYrmyJEjxS8m/wNr3ry56LOj9PTpU/GLzL0vePpp7969ReClxL0veFov74P/MfE/GK2lS5dKZcqUEe8TT1vlvj6ZwdD58YV7I8n4H+pHH30kppHyP7o33nhDBFFKN2/elFq3bi16b/AfAP4ii4+P13s/q1atKs4hKChI9RpZ+fvA02y5rwzvk/+A8uckB0e54fzMDZByw3nyFG1/f3+xX/63wreV/YFywzmytWvXii9//ptQtmxZ6a+//lI9nhv+9jDu78R/b7THnls+y/DwcPFvkPft5OQkXp976ymn4+e0zzIf/yd9OScAAACA3A01SAAAAAAaCJAAAAAANBAgAQAAAGggQAIAAADQQIAEAAAAoIEACQAAAEADARIAAACABgIkALAqN2/epHz58ollQ6zFxYsXqW7dumJx3KpVq1r6cAAgGyBAAgCVXr16iQBl/PjxqvtXrVol7s+LRo8eTa6urmKNLO0aT7LHjx/TgAEDqGjRouTo6CjWnWrVqhXt27dPtw2/f/w+AoD1Q4AEAHo4U/Ljjz/S8+fPKbfgFdMziheJbdiwoVhE1cfHx+A2b731Fp04cYL+/vtvseDqmjVrqEmTJvT06dOXOGoAsBQESACgp0WLFiIDMm7cOKPbjBkzRm+4afLkyVS8eHFVNqpjx470ww8/UKFChcjT05O++eYbSkhIoGHDhpG3tzcVKVJErHJuaFirfv36IlirWLEi7dq1S/X42bNnqXXr1pQ/f36x7+7du9OTJ090j3Nw8vHHH9PgwYOpQIECIptjCK8UzsfEx8GZHz6njRs3qrI+vLo4b8PX+by1eAXyPXv2iKCyadOmIpCqXbs2jRgxgtq3by+2kd+XN954Q+xH+T6tXr2aqlevLs41KCiIxo4dK94j5TFMmzZNnK+zs7PYZvny5argj8/V399f7INf39RnBwBpQ4AEAHpsbW1FUPPbb7/R3bt3X2pf27dvp/v379Pu3bvpl19+EcNVr7/+Onl5edGhQ4foww8/pA8++EDvdTiA+vzzz0VWpl69etSuXTtdNoYDkmbNmlG1atXo6NGjIqB5+PAhde7cWbUPzuY4ODiIYa7p06cbPL4pU6bQzz//TBMnTqTTp0+LQIqDmitXrojHQ0JCqEKFCuJY+PrQoUP19sFBGl94+Cw2Ntbg6xw5ckT85GCQ9yPf5sCqR48eNGjQIDp//jz9+eefNHfuXPr+++9Vzx85cqTIUp06dYreffdd6tq1K124cEE89uuvv4qM1dKlS8Uw4MKFC1UBGABkQLqXtwWAXK1nz55Shw4dxPW6detKffr0EddXrlwpViOXjR49WqpSpYrquZMmTZKKFSum2hffTkxM1N0XHBwsNWrUSHc7ISFBcnV1lRYvXixu37hxQ7yOcoVuXrW8SJEi0o8//ihuf/vtt1LLli1Vr33nzh3VaumvvPKKVK1atTTPNyAgQPr+++9V99WqVUusri7j8+TzNWX58uViNXZeybx+/frSiBEjxKrjSnx8/D4q8YrmP/zwg+q++fPnS/7+/qrnffjhh6pt6tSpIw0YMEBc/+STT6RmzZqJ1dIBIHMggwQARvGQEWdh5ExFRnD2xcYm9U8ND4dVqlRJla3iup5Hjx6pnsdZI5mdnR3VrFlTdxycRdmxY4cuc8OXsmXL6uqFZDVq1DB5bOHh4SK71aBBA9X9fDu958zZHd4XZ3Jee+012rlzpxg242yQKXwuPHynPJf+/fuLLFNUVJTB90O+LR8jD2XyrL/g4GD69NNPafPmzek6dgDQZ2fgPgAAoXHjxmLIiWtp+EtYiYOe5ORGqvj4eL192Nvbq25zPY2h+7gWyFwRERFiyI0DOC2uw5HxzLPsxPU/r776qrjwkFi/fv3EkKL2vdOeC9ccvfnmmwb3Zw4OxG7cuEEbNmygrVu3iqFGriNT1ikBQPoggwQAJvF0/7Vr19KBAwdU9xcsWJAePHigCpIys3fRwYMHdde5YJkLpcuVK6cLCM6dOyfqbEqVKqW6pCcocnd3p4CAANVUfMa3y5cv/9LnwPuIjIzU3ebAMDExUbUNnwvXDWnPgy/KzJvy/ZBvy++HfC5dunShGTNm0D///EP//vsvPXv27KXPASCvQgYJAEzi4TAuCuZCYCWeJca9fyZMmEBvv/22KJTmDAZ/UWeGqVOnUunSpUUQMGnSJNFyoE+fPuKxgQMHikCgW7du9MUXX4jZcFevXqUlS5bQzJkzxbCdubgYnLM8JUuWFDPYuIiaAz0udDYXF4936tRJHF/lypXJzc1NFI/ze9OhQwfddhzQcR8lHsLjGXNcqD5q1ChRtM79k/h95KCIh914lt53332ne+6yZcvEMCO3G+BjO3z4MM2aNUs8xsXvnDnjonV+Pm/LsxB51iAAZAwySACQJq6R0Q6BceDyxx9/iECmSpUq4gvb0Ayvl8lc8YX3vXfvXlHbw9P1mZz14WxMy5YtRRDH0/k5IFBmXczBNTtDhgwRs9R4Pxzo8WtxcGYurhuqU6eOCOR4WJLbEvAQG9cS/f7777rteLbcli1bKDAwUAQzjIcw161bJ+qGatWqJTp28354qr4SD8NxAMgB2Lx582jx4sW6LBcHZByMcQDF++Bu5OvXr0/3ewEAqfJxpbbiNgAAWBmu0Vq5cqXoKQUA2QP/ewEAAACggQAJAAAAQANF2gAAVg6VEADZDxkkAAAAAA0ESAAAAAAaCJAAAAAANBAgAQAAAGggQAIAAADQQIAEAAAAoIEACQAAAEADARIAAACABgIkAAAAAFL7P4qL1xIJrGtgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training data\n",
    "plt.plot(torch.arange(0, len(cost_data))*flatten_const, cost_data)\n",
    "plt.xlabel('Number of Steps')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Average Loss vs Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "<episode>\n",
      "scene:  pennys apartment.\n",
      "\n",
      "leonard:  hair until he on the cheese is been and got you tried on the point, and i was mad having it for her for warries.\n",
      "\n",
      "penny:  i flant matter. and this is mad. i never can draw him she last bange it is a little in diaser others like, and i used to worry with you. priya didnt to call.\n",
      "\n",
      "sheldon:  oh, good look. and one shamy liwy cheese and extrated, not lie of its only or.\n",
      "\n",
      "penny:  hey, its not comped.\n",
      "\n",
      "sheldon:  its okay.\n",
      "\n",
      "penny:  why do i be worried into the authing computes?\n",
      "\n",
      "sheldon:  ill see you out each for the mandaleagual is as president the stutence of by head. im trouble, go people has talking about his baod-neodled slow dishermo.\n",
      "\n",
      "penny:  im not your friend.\n",
      "\n",
      "leonard:  i was dested.\n",
      "\n",
      "sheldon:  esweet, ill take you to live up. we can nice who had matterable. how is in the way the laberrie, and his turn of gring leonard has of a proble knight years but the comic books are every elf, but i thought you as impled all webing the next more at all night day of what?\n",
      "\n",
      "leonard:  do you know it was just a little old?\n",
      "\n",
      "sheldon:  um, i can involvate the lost test computed in the changes contratula and he clean fell a little on party?\n",
      "\n",
      "leonard:  a few call. were actually early dicked.\n",
      "\n",
      "sheldon:  oh. its right. i guess that school ord. now, or, i am the own that either. and this is coming. dont worry out of women because they get back in a try tooable that.\n",
      "\n",
      "sheldon:  thats with my first athem.\n",
      "\n",
      "amy:  oh, its right.\n",
      "\n",
      "sheldon:  oh, i have about any god, sheldon.\n",
      "\n",
      "sheldon:  hang on.\n",
      "\n",
      "penny:  okay, dear, yeah, i was sitting it on this did if i get your mother.\n",
      "\n",
      "penny:  come on, why not?\n",
      "\n",
      "sheldon:  yes, the mind, i did it you?\n",
      "\n",
      "penny:  oh, i really want to get it. the contact, interesting. and, its just gonna be in. one on from cake her lab.\n",
      "\n",
      "leonard:  oh, well, and its trying to convor on lanteranel to have he experiments angement.\n",
      "\n",
      "sheldon:  what was that a down space? (the shame is reading a research wairthday.) \n",
      "\n",
      "sheldon:  its a good mother for the aevologies, but i had a samom bills trag back to do fat working on a dinner room, and he reshes got madakes me freelickets writebousinal after a little fur matter seking me should list to my train turn.\n",
      "\n",
      "amy:  rosters. would it say anytically from hot dude you at all my friendship for my couse?\n",
      "\n",
      "sheldon:  leonard, or is that he scared. everybody scientific, i would have been the future to have some of you werent an until the other freeze hot again from who has a show to prode painta talking matter train, a coddle and recentrair. and i can have had your back at it, i used to the night build-intimate. wratter has a dryyear-own, right? \n",
      "\n",
      "penny:  hey, dont worry. well just because you have a lot on year the geis extrasa shes us back together. im so early and we sign goah this glass and shamont.\n",
      "\n",
      "penny:  oh. sounds right! so, that is not for in the matast of working on a third.\n",
      "\n",
      "leonard:  why did this that you were on cat?\n",
      "\n",
      "sheldon:  not so, what are you?\n",
      "\n",
      "leonard:  okay, we have everything driver me.\n",
      "\n",
      "sheldon:  its serious fun. latern gamatte on neither considere tree that neighs figures. i may work about givill a toach for its truded to manapheat in the ring thing. shes gonna look late.\n",
      "\n",
      "howard:  whech never back, how is your last a jade in used to to sell howard holes brant these?\n",
      "\n",
      "flanding):  or ill get your bus love dictated oney.\n",
      "\n",
      "sheldon:  what do you have an idiot?\n",
      "\n",
      "leonard:  because they didnt talk to her in a progred ope. of court, mrs and cheers one years foult your meton we want to crazy.\n",
      "\n",
      "wunting):  i was so on we are this hard half. hes been fingering. youve had been the aid to the lab.\n",
      "\n",
      "leonard:  and thats a deal robbite on the termination cheese and profectical as restaurant. feel id have alreaded like a letting with eyes at care.\n",
      "\n",
      "howard:  yeah.\n",
      "\n",
      "raj:  god sorry, i tell her you?\n",
      "\n",
      "howard:  whats going?\n",
      "\n",
      "sheldon:  but i got what?\n",
      "\n",
      "howard:  its not nice. thats worried by us, its so an arguing conditional burnty and it outside, and id i hear with me!\n",
      "\n",
      "sheldon:  its very jean to poll.\n",
      "\n",
      "leonard:  lets her happy.\n",
      "\n",
      "howard:  i believe it.\n",
      "\n",
      "sheldon:  okay, did you repain have in betters to bind mind out on the bar of message that in the projectiods would be plassing a his devening. one tocker do.\n",
      "\n",
      "penny:  so, so, howard, listen, guys.\n",
      "\n",
      "amy:  so, please. i dont, this is pretty mind. i dont have a cruck every of romant research was without the jeing along and the and movemace. its leonard.\n",
      "\n",
      "hawking:  shes a little of a jeweek with me.\n",
      "\n",
      "leonard:  why happen?\n",
      "\n",
      "sheldon:  countrather. oh, did you please only asapplidays on stoppart cardongaked special i find and can heat glong as to me as so such a rough of hearting around on top a hological, a research tsome guy hole brided pyjamanipating. i got it a punkle story one last best house. who makes penny be reading and i may have two fingers who glad hangs some time cricket would be a gravy let i took some forms.\n",
      "\n",
      "together:  whats red together?\n",
      "\n",
      "howard:  yeah. she thinks you were fin\"\n",
      "\n",
      "[Perplexity of generated text: 3.020]\n"
     ]
    }
   ],
   "source": [
    "# LLM Inference\n",
    "\n",
    "def sample_with_temp(probs, temperature=1.0, top_k=None):\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = probs ** (1.0 / temperature)\n",
    "    probs = probs / probs.sum()  # Re-normalize\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Get top-k probabilities and their indices\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        top_probs = top_probs / top_probs.sum()\n",
    "        sampled_index = torch.multinomial(top_probs, num_samples=1)\n",
    "        return top_indices[sampled_index].item()\n",
    "    else:\n",
    "        return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "# Begin generation\n",
    "model.eval()\n",
    "tokens = data._encoded_train_data[-data.ctx_size:]\n",
    "text = data.train_text[0:data.ctx_size]\n",
    "print('\"', end='', sep='')\n",
    "\n",
    "total_log_prob = 0.0\n",
    "n_tokens = 0\n",
    "\n",
    "# Sampling config\n",
    "TEMPERATURE = 0.8\n",
    "TOP_K = 40\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5000):\n",
    "        # Generate input\n",
    "        x = torch.tensor(tokens[-data.ctx_size:]).to(device)\n",
    "        x = x.reshape((1, data.ctx_size))\n",
    "\n",
    "        # Model forward\n",
    "        output = model(x)  # shape: (1, T, vocab_size)\n",
    "        output = output.view((chunk_size, len(tokenizer.vocab_list)))  # Flattening, okay here\n",
    "        logits = output[-1]  # Last token's prediction\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample a token\n",
    "        #sampled_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        sampled_token = sample_with_temp(probs, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "        tokens.append(sampled_token)\n",
    "\n",
    "        # Track log probability\n",
    "        log_prob = torch.log(probs[sampled_token])\n",
    "        total_log_prob += log_prob.item()\n",
    "        n_tokens += 1\n",
    "\n",
    "        # Decode and print\n",
    "        char = tokenizer.decode([sampled_token])[0]\n",
    "        text += char\n",
    "        print(char, end='', sep='')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print('\"\\n', sep='')\n",
    "\n",
    "# Compute and print perplexity\n",
    "import math\n",
    "perplexity = math.exp(-total_log_prob / n_tokens)\n",
    "print(f\"[Perplexity of generated text: {perplexity:.3f}]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
